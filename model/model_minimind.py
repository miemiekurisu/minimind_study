"""
MiniMind å¤§è¯­è¨€æ¨¡å‹å®ç°æ–‡ä»¶
===========================================

æœ¬æ–‡ä»¶å®ç°äº†ä¸€ä¸ªè½»é‡çº§çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆMiniMindï¼‰ï¼ŒåŸºäºTransformeræ¶æ„ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1. æ”¯æŒå› æœè¯­è¨€å»ºæ¨¡ï¼ˆCausal Language Modelingï¼‰
2. ä½¿ç”¨RMSNormæ›¿ä»£LayerNormè¿›è¡Œå½’ä¸€åŒ–
3. é‡‡ç”¨RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰å¤„ç†åºåˆ—ä½ç½®ä¿¡æ¯
4. æ”¯æŒåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGrouped Query Attention, GQAï¼‰
5. å¯é€‰çš„ä¸“å®¶æ··åˆï¼ˆMixture of Experts, MoEï¼‰æ¶æ„
6. å…¼å®¹Hugging Face transformersåº“

æ–‡ä»¶ç»“æ„æ¦‚è§ˆï¼š
â”œâ”€â”€ MiniMindConfig: æ¨¡å‹é…ç½®ç±»ï¼Œå®šä¹‰æ‰€æœ‰è¶…å‚æ•°
â”œâ”€â”€ åŸºç¡€ç»„ä»¶:
â”‚   â”œâ”€â”€ RMSNorm: æ ¹å‡æ–¹å½’ä¸€åŒ–å±‚
â”‚   â”œâ”€â”€ precompute_freqs_cis: é¢„è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç 
â”‚   â””â”€â”€ apply_rotary_pos_emb: åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
â”œâ”€â”€ æ ¸å¿ƒæ¨¡å—:
â”‚   â”œâ”€â”€ Attention: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆæ”¯æŒFlash Attentionï¼‰
â”‚   â”œâ”€â”€ FeedForward: æ ‡å‡†å‰é¦ˆç½‘ç»œ
â”‚   â”œâ”€â”€ MOEFeedForward: ä¸“å®¶æ··åˆå‰é¦ˆç½‘ç»œ
â”‚   â””â”€â”€ MiniMindBlock: Transformerå—
â”œâ”€â”€ å®Œæ•´æ¨¡å‹:
â”‚   â”œâ”€â”€ MiniMindModel: ä¸»æ¨¡å‹ç±»
â”‚   â””â”€â”€ MiniMindForCausalLM: ç”¨äºå› æœè¯­è¨€å»ºæ¨¡çš„åŒ…è£…ç±»

æ¨¡å‹æ¶æ„ç‰¹ç‚¹ï¼š
- ä½¿ç”¨SwiGLUæ¿€æ´»å‡½æ•°çš„å‰é¦ˆç½‘ç»œ
- æ”¯æŒKVç¼“å­˜çš„é«˜æ•ˆæ¨ç†
- å¯é€‰çš„Flash AttentionåŠ é€Ÿ
- æƒé‡å…±äº«çš„è¯åµŒå…¥å’Œè¾“å‡ºå±‚
- æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦è®­ç»ƒ

PyTorchåº“å’Œå‡½æ•°ä½¿ç”¨è¯´æ˜ï¼š
============================

1. æ ¸å¿ƒPyTorchæ¨¡å—ï¼š
   - torch.nn.Module: æ‰€æœ‰ç¥ç»ç½‘ç»œå±‚çš„åŸºç±»ï¼Œç”¨äºå®šä¹‰æ¨¡å‹ç»„ä»¶
   - torch.nn.Parameter: å¯å­¦ä¹ å‚æ•°çš„åŒ…è£…å™¨ï¼Œä¼šè‡ªåŠ¨åŠ å…¥æ¢¯åº¦è®¡ç®—
   - torch.nn.Linear: çº¿æ€§å˜æ¢å±‚ï¼Œç”¨äºæŠ•å½±å’Œåˆ†ç±»å¤´
   - torch.nn.Embedding: è¯åµŒå…¥å±‚ï¼Œå°†token IDè½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
   - torch.nn.ModuleList: æ¨¡å—åˆ—è¡¨å®¹å™¨ï¼Œç”¨äºå­˜å‚¨å¤šä¸ªTransformerå±‚
   - torch.nn.Dropout: éšæœºå¤±æ´»å±‚ï¼Œç”¨äºæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ

2. æ¿€æ´»å‡½æ•°å’Œå½’ä¸€åŒ–ï¼š
   - torch.nn.functional.silu/gelu: SiLUå’ŒGELUæ¿€æ´»å‡½æ•°ï¼Œç”¨äºå‰é¦ˆç½‘ç»œ
   - torch.rsqrt: å¹³æ–¹æ ¹å€’æ•°ï¼Œç”¨äºRMSNormçš„é«˜æ•ˆè®¡ç®—
   - torch.nn.functional.layer_norm: å±‚å½’ä¸€åŒ–ï¼ˆæœ¬æ–‡ä»¶ä½¿ç”¨è‡ªå®šä¹‰RMSNormï¼‰

3. æ³¨æ„åŠ›æœºåˆ¶ç›¸å…³ï¼š
   - torch.nn.functional.scaled_dot_product_attention: Flash Attentionçš„PyTorchå®ç°
   - torch.nn.functional.softmax: softmaxæ¿€æ´»ï¼Œç”¨äºæ³¨æ„åŠ›æƒé‡è®¡ç®—
   - torch.transpose: å¼ é‡è½¬ç½®ï¼Œç”¨äºè°ƒæ•´æ³¨æ„åŠ›å¤´çš„ç»´åº¦é¡ºåº
   - torch.bmm/@: æ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼Œç”¨äºæ³¨æ„åŠ›åˆ†æ•°å’Œå€¼çš„è®¡ç®—

4. å¼ é‡æ“ä½œï¼š
   - torch.cat: å¼ é‡æ‹¼æ¥ï¼Œç”¨äºKVç¼“å­˜å’Œä½ç½®ç¼–ç 
   - torch.split/chunk: å¼ é‡åˆ†å‰²ï¼Œç”¨äºå¤šå¤´æ³¨æ„åŠ›çš„å¤´åˆ†ç¦»
   - torch.view/reshape: å¼ é‡å½¢çŠ¶å˜æ¢ï¼Œç”¨äºç»´åº¦è°ƒæ•´
   - torch.unsqueeze/squeeze: å¢å‡å¼ é‡ç»´åº¦
   - torch.repeat_interleave: é‡å¤å¼ é‡å…ƒç´ ï¼Œç”¨äºGQAä¸­çš„KVå¤åˆ¶

5. æ•°å­¦è¿ç®—ï¼š
   - torch.outer: å¤–ç§¯è¿ç®—ï¼Œç”¨äºç”Ÿæˆä½ç½®ç¼–ç é¢‘ç‡çŸ©é˜µ
   - torch.cos/sin: ä¸‰è§’å‡½æ•°ï¼Œç”¨äºæ—‹è½¬ä½ç½®ç¼–ç 
   - torch.sqrt: å¹³æ–¹æ ¹ï¼Œç”¨äºæ³¨æ„åŠ›ç¼©æ”¾
   - torch.triu: ä¸Šä¸‰è§’çŸ©é˜µï¼Œç”¨äºå› æœæ³¨æ„åŠ›æ©ç 

6. ä¸“å®¶æ··åˆï¼ˆMoEï¼‰ç›¸å…³ï¼š
   - torch.topk: é€‰æ‹©top-kä¸“å®¶
   - torch.scatter_add_: åŸåœ°æ•£åˆ—åŠ æ³•ï¼Œç”¨äºä¸“å®¶è¾“å‡ºèšåˆ
   - torch.bincount: è®¡æ•°æ“ä½œï¼Œç”¨äºä¸“å®¶è´Ÿè½½å¹³è¡¡
   - torch.argsort: æ’åºç´¢å¼•ï¼Œç”¨äºä¸“å®¶æ¨ç†ä¼˜åŒ–

7. ç¼“å­˜å’Œå†…å­˜ç®¡ç†ï¼š
   - register_buffer: æ³¨å†Œä¸å¯è®­ç»ƒçš„æ¨¡å‹çŠ¶æ€ï¼ˆå¦‚ä½ç½®ç¼–ç ï¼‰
   - torch.no_grad: ç¦ç”¨æ¢¯åº¦è®¡ç®—çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºæ¨ç†ä¼˜åŒ–

8. å…¼å®¹æ€§å’Œé›†æˆï¼š
   - transformers.PreTrainedModel: Hugging Faceæ¨¡å‹åŸºç±»
   - transformers.GenerationMixin: ç”Ÿæˆä»»åŠ¡çš„æ··åˆç±»
   - transformers.modeling_outputs.CausalLMOutputWithPast: æ ‡å‡†åŒ–çš„æ¨¡å‹è¾“å‡ºæ ¼å¼

Transformersåº“è¯¦ç»†è¯´æ˜ï¼š
============================

Transformersåº“æ˜¯Hugging Faceå¼€å‘çš„è‡ªç„¶è¯­è¨€å¤„ç†åº“ï¼Œæä¾›äº†é¢„è®­ç»ƒæ¨¡å‹å’Œå·¥å…·ã€‚
æœ¬æ–‡ä»¶ä¸»è¦ä½¿ç”¨ä»¥ä¸‹ç»„ä»¶ï¼š

1. é…ç½®åŸºç±» (transformers.PretrainedConfig)ï¼š
   - ä½œç”¨ï¼šæä¾›æ¨¡å‹é…ç½®çš„æ ‡å‡†åŒ–åŸºç±»
   - åŠŸèƒ½ï¼šæ”¯æŒé…ç½®çš„ä¿å­˜ã€åŠ è½½å’ŒJSONåºåˆ—åŒ–
   - ç”¨æ³•ï¼šMiniMindConfigç»§æ‰¿æ­¤ç±»ï¼Œè·å¾—æ ‡å‡†é…ç½®ç®¡ç†èƒ½åŠ›
   - ç¤ºä¾‹ï¼šconfig.save_pretrained(), config.from_pretrained()

2. æ¨¡å‹åŸºç±» (transformers.PreTrainedModel)ï¼š
   - ä½œç”¨ï¼šæ‰€æœ‰Hugging Faceæ¨¡å‹çš„åŸºç±»ï¼Œæä¾›æ ‡å‡†åŒ–æ¥å£
   - åŠŸèƒ½ï¼šæ¨¡å‹ä¿å­˜/åŠ è½½ã€è®¾å¤‡ç®¡ç†ã€æ¢¯åº¦æ£€æŸ¥ç‚¹ç­‰
   - ç”¨æ³•ï¼šMiniMindForCausalLMç»§æ‰¿æ­¤ç±»ï¼Œè·å¾—å®Œæ•´æ¨¡å‹ç®¡ç†èƒ½åŠ›
   - å…³é”®æ–¹æ³•ï¼š
     * save_pretrained(): ä¿å­˜æ¨¡å‹æƒé‡å’Œé…ç½®
     * from_pretrained(): ä»é¢„è®­ç»ƒæƒé‡åŠ è½½æ¨¡å‹
     * to(): è®¾å¤‡è½¬ç§»ï¼ˆCPU/GPUï¼‰
     * train()/eval(): è®­ç»ƒ/è¯„ä¼°æ¨¡å¼åˆ‡æ¢
     * parameters(): è·å–å¯è®­ç»ƒå‚æ•°

3. ç”Ÿæˆæ··å…¥ç±» (transformers.GenerationMixin)ï¼š
   - ä½œç”¨ï¼šä¸ºè¯­è¨€æ¨¡å‹æä¾›æ–‡æœ¬ç”ŸæˆåŠŸèƒ½
   - åŠŸèƒ½ï¼šæ”¯æŒå¤šç§ç”Ÿæˆç­–ç•¥å’Œè§£ç ç®—æ³•
   - ç”¨æ³•ï¼šMiniMindForCausalLMç»§æ‰¿æ­¤ç±»ï¼Œè·å¾—generate()æ–¹æ³•
   - ç”Ÿæˆç­–ç•¥ï¼š
     * è´ªå©ªæœç´¢ï¼šgenerate(do_sample=False)
     * é‡‡æ ·ç”Ÿæˆï¼šgenerate(do_sample=True, temperature=0.7)
     * æŸæœç´¢ï¼šgenerate(num_beams=4)
     * top-ké‡‡æ ·ï¼šgenerate(do_sample=True, top_k=50)
     * top-pé‡‡æ ·ï¼šgenerate(do_sample=True, top_p=0.9)

4. è¾“å‡ºæ ¼å¼ç±» (transformers.modeling_outputs.CausalLMOutputWithPast)ï¼š
   - ä½œç”¨ï¼šæ ‡å‡†åŒ–å› æœè¯­è¨€æ¨¡å‹çš„è¾“å‡ºæ ¼å¼
   - åŠŸèƒ½ï¼šå°è£…logitsã€hidden_statesã€past_key_valuesç­‰è¾“å‡º
   - ç”¨æ³•ï¼šforward()æ–¹æ³•è¿”å›æ­¤ç±»å‹ï¼Œå…¼å®¹HFç”Ÿæ€
   - å±æ€§ï¼š
     * logits: é¢„æµ‹çš„è¯æ±‡è¡¨åˆ†å¸ƒ
     * past_key_values: KVç¼“å­˜ï¼Œç”¨äºå¢é‡ç”Ÿæˆ
     * hidden_states: å„å±‚éšè—çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
     * attentions: æ³¨æ„åŠ›æƒé‡ï¼ˆå¯é€‰ï¼‰

5. æ¿€æ´»å‡½æ•°æ˜ å°„ (transformers.activations.ACT2FN)ï¼š
   - ä½œç”¨ï¼šæä¾›æ ‡å‡†åŒ–çš„æ¿€æ´»å‡½æ•°æ˜ å°„å­—å…¸
   - åŠŸèƒ½ï¼šé€šè¿‡å­—ç¬¦ä¸²åç§°è·å–å¯¹åº”çš„æ¿€æ´»å‡½æ•°
   - ç”¨æ³•ï¼šACT2FN[config.hidden_act] è·å–é…ç½®çš„æ¿€æ´»å‡½æ•°
   - æ”¯æŒå‡½æ•°ï¼š
     * "relu": torch.nn.functional.relu
     * "gelu": torch.nn.functional.gelu
     * "silu": torch.nn.functional.silu (Swish)
     * "swish": torch.nn.functional.silu
     * "tanh": torch.tanh

Transformersåº“é›†æˆä¼˜åŠ¿ï¼š
1. ç”Ÿæ€å…¼å®¹æ€§ï¼šä¸Hugging Face Hubã€datasetsã€tokenizersç­‰æ— ç¼é›†æˆ
2. æ ‡å‡†åŒ–æ¥å£ï¼šç»Ÿä¸€çš„APIè®¾è®¡ï¼Œä¾¿äºæ¨¡å‹æ›¿æ¢å’Œæ¯”è¾ƒ
3. ç¤¾åŒºæ”¯æŒï¼šä¸°å¯Œçš„é¢„è®­ç»ƒæ¨¡å‹å’Œç¤¾åŒºè´¡çŒ®
4. éƒ¨ç½²ä¾¿åˆ©ï¼šæ”¯æŒONNXå¯¼å‡ºã€TensorRTä¼˜åŒ–ç­‰éƒ¨ç½²æ–¹æ¡ˆ
5. ç‰ˆæœ¬ç®¡ç†ï¼šå®Œå–„çš„æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶å’Œé…ç½®ç®¡ç†
6. å¼€ç®±å³ç”¨ï¼šæä¾›å®Œæ•´çš„è®­ç»ƒã€æ¨ç†å’Œè¯„ä¼°å·¥å…·é“¾

ä¸»è¦æŠ€æœ¯äº®ç‚¹ï¼š
- é«˜æ•ˆçš„æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰å®ç°
- åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰å‡å°‘KVç¼“å­˜å†…å­˜å ç”¨
- å¯é€‰çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„æå‡æ¨¡å‹å®¹é‡
- å…¼å®¹Flash Attentionçš„é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—
- å®Œæ•´çš„KVç¼“å­˜æ”¯æŒå®ç°æµå¼ç”Ÿæˆ


"""

# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Config
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

# Transformersåº“å¯¼å…¥è¯´æ˜ï¼š
# PretrainedConfig: Hugging Faceé…ç½®åŸºç±»ï¼Œæä¾›é…ç½®çš„æ ‡å‡†åŒ–ç®¡ç†
#   - æ”¯æŒJSONåºåˆ—åŒ–å’Œååºåˆ—åŒ–
#   - æä¾›save_pretrained()å’Œfrom_pretrained()æ–¹æ³•
#   - è‡ªåŠ¨å¤„ç†é…ç½®å‚æ•°çš„éªŒè¯å’Œé»˜è®¤å€¼è®¾ç½®
from transformers import PretrainedConfig


class MiniMindConfig(PretrainedConfig):
    """
    MiniMindæ¨¡å‹é…ç½®ç±»
    ==================
    
    é¡¹ç›®ä¸­çš„ä½œç”¨ï¼š
    - ä½œä¸ºæ•´ä¸ªMiniMindé¡¹ç›®çš„é…ç½®ä¸­å¿ƒï¼Œç»Ÿä¸€ç®¡ç†æ‰€æœ‰è¶…å‚æ•°
    - æä¾›æ¨¡å‹è®­ç»ƒã€æ¨ç†å’Œéƒ¨ç½²æ‰€éœ€çš„å®Œæ•´é…ç½®ä¿¡æ¯
    - æ”¯æŒä¸åŒè§„æ¨¡æ¨¡å‹çš„çµæ´»é…ç½®ï¼ˆ512ç»´ã€768ç»´ç­‰ï¼‰
    
    å¤§æ¨¡å‹æ¡†æ¶ä¸­çš„ä½œç”¨ï¼š
    - ç»§æ‰¿è‡ªHugging Faceçš„PretrainedConfigï¼Œç¡®ä¿ä¸transformersç”Ÿæ€å…¼å®¹
    - å®ç°æ¨¡å‹é…ç½®çš„æ ‡å‡†åŒ–å­˜å‚¨å’ŒåŠ è½½æœºåˆ¶
    - æ”¯æŒæ¨¡å‹é…ç½®çš„ç‰ˆæœ¬æ§åˆ¶å’Œå¤ç°æ€§ä¿è¯
    - ä¸ºä¸åŒçš„è®­ç»ƒå’Œæ¨ç†åœºæ™¯æä¾›é…ç½®æ¨¡æ¿
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. åŸºç¡€æ¶æ„é…ç½®ï¼šå±‚æ•°ã€å¤´æ•°ã€éšè—ç»´åº¦ç­‰Transformeræ ¸å¿ƒå‚æ•°
    2. é«˜çº§ç‰¹æ€§é…ç½®ï¼šGQAã€Flash Attentionã€RoPEç­‰ç°ä»£æŠ€æœ¯å¼€å…³
    3. MoEä¸“å®¶æ··åˆé…ç½®ï¼šä¸“å®¶æ•°é‡ã€è·¯ç”±ç­–ç•¥ã€è´Ÿè½½å¹³è¡¡ç­‰
    4. è®­ç»ƒä¼˜åŒ–é…ç½®ï¼šdropoutã€å½’ä¸€åŒ–å‚æ•°ç­‰æ­£åˆ™åŒ–è®¾ç½®
    
    ä½¿ç”¨åœºæ™¯ï¼š
    - æ¨¡å‹åˆå§‹åŒ–æ—¶å®šä¹‰æ¶æ„å‚æ•°
    - è®­ç»ƒè„šæœ¬ä¸­é…ç½®è¶…å‚æ•°
    - æ¨¡å‹ä¿å­˜æ—¶è®°å½•å®Œæ•´é…ç½®
    - æ¨ç†éƒ¨ç½²æ—¶æ¢å¤æ¨¡å‹è®¾ç½®
    """
    # model_typeè¯¦ç»†è¯´æ˜ï¼š
    # è¿™æ˜¯transformersåº“è¦æ±‚çš„æ¨¡å‹ç±»å‹æ ‡è¯†ç¬¦
    # ç”¨äºåœ¨æ¨¡å‹æ³¨å†Œå’Œè‡ªåŠ¨åŠ è½½æ—¶è¯†åˆ«æ¨¡å‹ç±»å‹
    # å¿…é¡»æ˜¯å”¯ä¸€çš„å­—ç¬¦ä¸²ï¼Œé€šå¸¸ä¸æ¨¡å‹åç§°å¯¹åº”
    # åœ¨ä½¿ç”¨AutoModel.from_pretrained()æ—¶ä¼šæ ¹æ®æ­¤æ ‡è¯†ç¬¦é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹ç±»
    model_type = "minimind"

    def __init__(
            self,
            dropout: float = 0.0,
            bos_token_id: int = 1,
            eos_token_id: int = 2,
            hidden_act: str = 'silu',
            hidden_size: int = 512,           # ğŸ¯ é‡è¦æ¦‚å¿µï¼šä¸ºä»€ä¹ˆæ˜¯ int è€Œä¸æ˜¯çŸ©é˜µå½¢çŠ¶ï¼Ÿ
            # =====================================================================
            # âš ï¸ å¸¸è§ç–‘æƒ‘è§£ç­”ï¼šhidden_size ä¸ºä»€ä¹ˆæ˜¯æ•´æ•°è€Œä¸æ˜¯äºŒç»´å½¢çŠ¶ï¼Ÿ
            # 
            # ğŸ¤” é”™è¯¯ç†è§£ï¼šè®¤ä¸ºåº”è¯¥æ˜¯ [512, 512] è¿™æ ·çš„çŸ©é˜µå½¢çŠ¶
            # âœ… æ­£ç¡®ç†è§£ï¼šhidden_size=512 è¡¨ç¤ºæ¯ä¸ªtokençš„ç‰¹å¾å‘é‡ç»´åº¦
            # 
            # ğŸ“ å®Œæ•´çš„å¼ é‡ç»´åº¦è§£é‡Šï¼š
            # ----------------------
            # åœ¨Transformerä¸­ï¼Œæ•°æ®ä»¥3Då¼ é‡å½¢å¼æµåŠ¨ï¼š
            # [batch_size, sequence_length, hidden_size]
            # 
            # å…·ä½“ä¾‹å­ï¼š
            # - batch_size=4        (4ä¸ªæ ·æœ¬)
            # - sequence_length=128 (æ¯ä¸ªæ ·æœ¬128ä¸ªtoken)
            # - hidden_size=512     (æ¯ä¸ªtokenç”¨512ç»´å‘é‡è¡¨ç¤º)
            # - å®Œæ•´å½¢çŠ¶ï¼š[4, 128, 512]
            # 
            # ğŸ” hidden_size çš„çœŸå®å«ä¹‰ï¼š
            # ---------------------------
            # 1. ç‰¹å¾ç©ºé—´ç»´åº¦ï¼šæ¯ä¸ªtokençš„å‘é‡è¡¨ç¤ºæœ‰å¤šå°‘ä¸ªç»´åº¦
            # 2. æ¨¡å‹å®½åº¦ï¼šå†³å®šæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œè®¡ç®—å¤æ‚åº¦
            # 3. æŠ•å½±è¾“å…¥ï¼šæ‰€æœ‰çº¿æ€§å±‚çš„è¾“å…¥ç»´åº¦
            # 
            # ğŸ’¡ æƒé‡çŸ©é˜µçš„çœŸå®å½¢çŠ¶ï¼š
            # ----------------------
            # Qæƒé‡ï¼š[hidden_size, num_heads * head_dim] = [512, 8*64] = [512, 512]
            # Kæƒé‡ï¼š[hidden_size, num_kv_heads * head_dim] = [512, 2*64] = [512, 128]
            # Væƒé‡ï¼š[hidden_size, num_kv_heads * head_dim] = [512, 2*64] = [512, 128]
            # 
            # ğŸ¯ ç±»æ¯”ç†è§£ï¼š
            # -----------
            # æƒ³è±¡æ¯ä¸ªtokenæ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œhidden_size=512æ„å‘³ç€ï¼š
            # - æ¯ä¸ªå­¦ç”Ÿæœ‰512é¡¹ç‰¹å¾ï¼ˆèº«é«˜ã€ä½“é‡ã€å„ç§‘æˆç»©...ï¼‰
            # - ä¸æ˜¯è¯´æœ‰512ä¸ªå­¦ç”Ÿï¼Œè€Œæ˜¯æ¯ä¸ªå­¦ç”Ÿæœ‰512ä¸ªç‰¹å¾ç»´åº¦
            # - æƒé‡çŸ©é˜µåƒæ˜¯"ç‰¹å¾è½¬æ¢å™¨"ï¼Œå°†è¿™512ä¸ªç‰¹å¾è½¬æ¢æˆæ–°çš„ç‰¹å¾ç»„åˆ
            # =====================================================================
            intermediate_size: int = None,
            max_position_embeddings: int = 32768,
            num_attention_heads: int = 8,
            num_hidden_layers: int = 8,
            num_key_value_heads: int = 2,
            vocab_size: int = 6400,
            rms_norm_eps: float = 1e-05,
            rope_theta: int = 1000000.0,
            flash_attn: bool = True,
            ####################################################
            # Here are the specific configurations of MOE
            # When use_moe is false, the following is invalid
            ####################################################
            use_moe: bool = False,
            num_experts_per_tok: int = 2,
            n_routed_experts: int = 4,
            n_shared_experts: int = 1,
            scoring_func: str = 'softmax',
            aux_loss_alpha: float = 0.1,
            seq_aux: bool = True,
            norm_topk_prob: bool = True,
            **kwargs
    ):
        # super().__init__(**kwargs)è¯¦ç»†è¯´æ˜ï¼š
        # è°ƒç”¨çˆ¶ç±»PretrainedConfigçš„åˆå§‹åŒ–æ–¹æ³•
        # **kwargsåŒ…å«transformersåº“çš„æ ‡å‡†é…ç½®å‚æ•°ï¼š
        # - architectures: æ¨¡å‹æ¶æ„ä¿¡æ¯ï¼Œç”¨äºAutoModelè‡ªåŠ¨åŠ è½½
        # - torch_dtype: æ¨¡å‹æƒé‡çš„æ•°æ®ç±»å‹ (float16, float32ç­‰)
        # - use_cache: æ˜¯å¦é»˜è®¤ä½¿ç”¨KVç¼“å­˜
        # - tie_word_embeddings: æ˜¯å¦å…±äº«è¾“å…¥è¾“å‡ºåµŒå…¥æƒé‡
        # - pad_token_id, eos_token_id, bos_token_id: ç‰¹æ®Štokençš„ID
        # è¿™ç¡®ä¿äº†é…ç½®ç±»ä¸transformersç”Ÿæ€ç³»ç»Ÿçš„å®Œå…¨å…¼å®¹
        super().__init__(**kwargs)
        
        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.flash_attn = flash_attn
        ####################################################
        # Here are the specific configurations of MOE
        # When use_moe is false, the following is invalid
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts  # æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts  # å…±äº«ä¸“å®¶
        self.scoring_func = scoring_func  # è¯„åˆ†å‡½æ•°ï¼Œé»˜è®¤ä¸º'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # è¾…åŠ©æŸå¤±çš„alphaå‚æ•°
        self.seq_aux = seq_aux  # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«ä¸Šè®¡ç®—è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob  # æ˜¯å¦æ ‡å‡†åŒ–top-kæ¦‚ç‡


# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Model
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

import math
import torch
from torch import nn

# Transformersåº“å¯¼å…¥è¯´æ˜ï¼š
# 1. ACT2FN: æ¿€æ´»å‡½æ•°æ˜ å°„å­—å…¸ï¼Œå°†å­—ç¬¦ä¸²æ˜ å°„åˆ°å¯¹åº”çš„æ¿€æ´»å‡½æ•°
#    - æ”¯æŒçš„æ¿€æ´»å‡½æ•°: relu, gelu, silu, swish, tanhç­‰
#    - ç”¨æ³•: ACT2FN['silu'] è¿”å› torch.nn.functional.silu
#    - ä¾¿äºé…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨å­—ç¬¦ä¸²æŒ‡å®šæ¿€æ´»å‡½æ•°ç±»å‹
from transformers.activations import ACT2FN

from typing import Optional, Tuple, List, Union
import torch.nn.functional as F

# 2. PreTrainedModel: Hugging Faceæ¨¡å‹åŸºç±»
#    - æä¾›æ ‡å‡†åŒ–çš„æ¨¡å‹æ¥å£å’Œç®¡ç†åŠŸèƒ½
#    - åŒ…å«save_pretrained(), from_pretrained()ç­‰æ ¸å¿ƒæ–¹æ³•
#    - æ”¯æŒè®¾å¤‡ç®¡ç†ã€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€å‚æ•°ç»Ÿè®¡ç­‰åŠŸèƒ½
#
# 3. GenerationMixin: æ–‡æœ¬ç”ŸæˆåŠŸèƒ½æ··å…¥ç±»
#    - ä¸ºè¯­è¨€æ¨¡å‹æä¾›generate()æ–¹æ³•
#    - æ”¯æŒå¤šç§ç”Ÿæˆç­–ç•¥ï¼šè´ªå©ªæœç´¢ã€æŸæœç´¢ã€é‡‡æ ·ç­‰
#    - åŒ…å«ç”Ÿæˆé…ç½®ç®¡ç†å’Œè§£ç ç®—æ³•å®ç°
#
# 4. PretrainedConfig: é…ç½®åŸºç±»ï¼ˆé‡å¤å¯¼å…¥ï¼Œç”¨äºç±»å‹æ³¨è§£ï¼‰
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig

# 5. CausalLMOutputWithPast: å› æœè¯­è¨€æ¨¡å‹è¾“å‡ºæ ¼å¼ç±»
#    - æ ‡å‡†åŒ–çš„æ¨¡å‹è¾“å‡ºå®¹å™¨ï¼ŒåŒ…å«logitsã€past_key_valuesç­‰
#    - å…¼å®¹Hugging Faceç”Ÿæ€ç³»ç»Ÿçš„è¾“å‡ºæ ¼å¼
#    - æ”¯æŒKVç¼“å­˜ä¼ é€’å’Œæ‰¹é‡æ¨ç†ä¼˜åŒ–
from transformers.modeling_outputs import CausalLMOutputWithPast


class RMSNorm(torch.nn.Module):
    """
    æ ¹å‡æ–¹å½’ä¸€åŒ–ï¼ˆRoot Mean Square Normalizationï¼‰å±‚
    ===============================================
    
    é¡¹ç›®ä¸­çš„ä½œç”¨ï¼š
    - ä½œä¸ºMiniMindæ¨¡å‹çš„æ ‡å‡†å½’ä¸€åŒ–ç»„ä»¶ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„LayerNorm
    - åœ¨æ¯ä¸ªTransformerå—ä¸­è¿›è¡Œç‰¹å¾å½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹
    - æä¾›æ›´é«˜æ•ˆçš„è®¡ç®—å®ç°ï¼Œå‡å°‘è®­ç»ƒå’Œæ¨ç†çš„è®¡ç®—å¼€é”€
    
    å¤§æ¨¡å‹æ¡†æ¶ä¸­çš„ä½œç”¨ï¼š
    - ä»£è¡¨ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„å½’ä¸€åŒ–æŠ€æœ¯è¶‹åŠ¿
    - è¢«LLaMAã€GPT-NeoXç­‰ä¸»æµæ¨¡å‹å¹¿æ³›é‡‡ç”¨
    - æä¾›ç›¸æ¯”LayerNormæ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡
    - æ”¯æŒå¤§è§„æ¨¡æ¨¡å‹çš„ç¨³å®šè®­ç»ƒå’Œæ”¶æ•›
    
    æŠ€æœ¯åŸç†ï¼š
    1. ç®€åŒ–çš„å½’ä¸€åŒ–å…¬å¼ï¼š
       - ç§»é™¤LayerNormä¸­çš„å‡å€¼å‡æ³•æ“ä½œ
       - ä»…ä¿ç•™æ–¹å·®å½’ä¸€åŒ–å’Œå¯å­¦ä¹ ç¼©æ”¾
       - å…¬å¼ï¼šRMSNorm(x) = x / sqrt(mean(xÂ²) + Îµ) * weight
    
    2. è®¡ç®—ä¼˜åŒ–ï¼š
       - å‡å°‘è®¡ç®—æ­¥éª¤ï¼Œæå‡æ‰§è¡Œæ•ˆç‡
       - æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§ç‰¹æ€§
       - æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒä¼˜åŒ–
    
    3. å‚æ•°è®¾è®¡ï¼š
       - ä»…åŒ…å«å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°weight
       - åˆå§‹åŒ–ä¸ºå…¨1å‘é‡
       - ç›¸æ¯”LayerNormå‡å°‘äº†åç½®å‚æ•°
    
    æ€§èƒ½ä¼˜åŠ¿ï¼š
    1. è®¡ç®—æ•ˆç‡ï¼š
       - å‡å°‘çº¦25%çš„è®¡ç®—é‡
       - æ›´å¥½çš„GPUå¹¶è¡ŒåŒ–ç‰¹æ€§
       - æ”¯æŒé«˜æ•ˆçš„kernelå®ç°
    
    2. æ•°å€¼ç¨³å®šæ€§ï¼š
       - é¿å…å‡å€¼è®¡ç®—çš„æ•°å€¼è¯¯å·®
       - æ›´ç¨³å®šçš„æ¢¯åº¦ä¼ æ’­
       - é€‚åˆå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒ
    
    3. å†…å­˜å‹å¥½ï¼š
       - å‡å°‘ä¸­é—´è®¡ç®—ç»“æœçš„å­˜å‚¨
       - æ”¯æŒå†…å­˜ä¼˜åŒ–çš„åå‘ä¼ æ’­
       - é€‚åˆå¤§æ¨¡å‹å’Œé•¿åºåˆ—åœºæ™¯
    
    åº”ç”¨åœºæ™¯ï¼š
    - Pre-Norm Transformeræ¶æ„çš„æ ‡å‡†ç»„ä»¶
    - å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å½’ä¸€åŒ–å±‚
    - éœ€è¦è®¡ç®—æ•ˆç‡ä¼˜åŒ–çš„æ·±åº¦ç½‘ç»œ
    - æ··åˆç²¾åº¦è®­ç»ƒçš„ç¨³å®šåŒ–ç»„ä»¶
    
    ä¸LayerNormçš„å¯¹æ¯”ï¼š
    - LayerNorm: (x - mean(x)) / sqrt(var(x) + Îµ) * weight + bias
    - RMSNorm: x / sqrt(mean(xÂ²) + Îµ) * weight
    - ä¼˜åŠ¿ï¼šè®¡ç®—æ›´ç®€å•ï¼Œå‚æ•°æ›´å°‘ï¼Œç¨³å®šæ€§æ›´å¥½
    """
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        # epså‚æ•°ï¼šæ•°å€¼ç¨³å®šæ€§å¸¸æ•°ï¼Œé˜²æ­¢é™¤é›¶é”™è¯¯
        # è®­ç»ƒä½œç”¨ï¼šç¡®ä¿æ¢¯åº¦è®¡ç®—çš„ç¨³å®šæ€§ï¼Œé¿å…æ•°å€¼æº¢å‡º
        # æ¨ç†ä½œç”¨ï¼šä¿è¯å½’ä¸€åŒ–è®¡ç®—çš„æ•°å€¼ç²¾åº¦
        self.eps = eps
        
        # weightå‚æ•°ï¼šå¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ï¼Œå½¢çŠ¶ä¸º(dim,)
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ æ¯ä¸ªç‰¹å¾ç»´åº¦çš„æœ€ä¼˜ç¼©æ”¾å› å­ï¼Œå½±å“æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½
        # æ¨ç†ä½œç”¨ï¼šå¯¹å½’ä¸€åŒ–åçš„ç‰¹å¾è¿›è¡Œç¼©æ”¾ï¼Œæ¢å¤æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›
        # åˆå§‹åŒ–ä¸ºå…¨1ï¼šä¿è¯è®­ç»ƒåˆæœŸä¸æ”¹å˜è¾“å…¥åˆ†å¸ƒ
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        """
        æ‰§è¡ŒRMSå½’ä¸€åŒ–çš„æ ¸å¿ƒè®¡ç®—
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (..., dim)
            
        Returns:
            torch.Tensor: å½’ä¸€åŒ–åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        """
        # è®¡ç®—xÂ²çš„å‡å€¼ï¼šæ²¿æœ€åä¸€ä¸ªç»´åº¦è®¡ç®—ï¼Œä¿æŒå…¶ä»–ç»´åº¦ä¸å˜
        # è®­ç»ƒä½œç”¨ï¼šæä¾›ç¨³å®šçš„æ¢¯åº¦ä¼ æ’­è·¯å¾„ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
        # æ¨ç†ä½œç”¨ï¼šæ ‡å‡†åŒ–ç‰¹å¾åˆ†å¸ƒï¼Œæé«˜æ¨¡å‹å¯¹è¾“å…¥å˜åŒ–çš„é²æ£’æ€§
        # keepdim=Trueï¼šä¿æŒç»´åº¦ç”¨äºåç»­å¹¿æ’­è¿ç®—
        variance = x.pow(2).mean(-1, keepdim=True)
        
        # torch.rsqrt()ï¼šè®¡ç®—å¹³æ–¹æ ¹çš„å€’æ•°ï¼Œç­‰ä»·äº1/sqrt(variance + eps)
        # è®­ç»ƒä½œç”¨ï¼šæ¯”å…ˆsqrtå†å–å€’æ•°æ›´é«˜æ•ˆï¼Œå‡å°‘æ•°å€¼è¯¯å·®ç´¯ç§¯
        # æ¨ç†ä½œç”¨ï¼šå¿«é€Ÿè®¡ç®—å½’ä¸€åŒ–å› å­ï¼Œä¼˜åŒ–æ¨ç†é€Ÿåº¦
        # åŠ epsé˜²æ­¢æ–¹å·®ä¸º0æ—¶çš„é™¤é›¶é”™è¯¯
        return x * torch.rsqrt(variance + self.eps)

    def forward(self, x):
        """
        RMSNormçš„å‰å‘ä¼ æ’­
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œé€šå¸¸ä¸ºéšè—çŠ¶æ€
            
        Returns:
            torch.Tensor: å½’ä¸€åŒ–å¹¶ç¼©æ”¾åçš„è¾“å‡ºå¼ é‡
        """
        # è½¬æ¢ä¸ºfloat32è¿›è¡Œè®¡ç®—ï¼šæé«˜æ•°å€¼ç¨³å®šæ€§
        # è®­ç»ƒä½œç”¨ï¼šé¿å…åŠç²¾åº¦è®­ç»ƒæ—¶çš„æ•°å€¼ä¸ç¨³å®šï¼Œç¡®ä¿æ¢¯åº¦è®¡ç®—ç²¾åº¦
        # æ¨ç†ä½œç”¨ï¼šåœ¨æ··åˆç²¾åº¦æ¨ç†ä¸­ä¿è¯å…³é”®è®¡ç®—çš„ç²¾åº¦
        normalized = self._norm(x.float())
        
        # åº”ç”¨å¯å­¦ä¹ æƒé‡å¹¶è½¬å›åŸå§‹ç±»å‹
        # è®­ç»ƒä½œç”¨ï¼šæƒé‡å‚æ•°æ¥æ”¶æ¢¯åº¦æ›´æ–°ï¼Œå­¦ä¹ æœ€ä¼˜çš„ç‰¹å¾ç¼©æ”¾
        # æ¨ç†ä½œç”¨ï¼šæ¢å¤æ¨¡å‹è®­ç»ƒæ—¶å­¦åˆ°çš„ç‰¹å¾åˆ†å¸ƒç‰¹æ€§
        # type_as(x)ï¼šç¡®ä¿è¾“å‡ºç±»å‹ä¸è¾“å…¥ä¸€è‡´ï¼Œæ”¯æŒæ··åˆç²¾åº¦
        return self.weight * normalized.type_as(x)


def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):
    """
    é¢„è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰çš„é¢‘ç‡çŸ©é˜µ
    
    RoPEé€šè¿‡æ—‹è½¬çš„æ–¹å¼å°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°æŸ¥è¯¢å’Œé”®å‘é‡ä¸­ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ç»å¯¹ä½ç½®ç¼–ç ï¼Œ
    RoPEå…·æœ‰æ›´å¥½çš„å¤–æ¨èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—ã€‚
    
    Args:
        dim (int): æ³¨æ„åŠ›å¤´çš„ç»´åº¦å¤§å°
        end (int): æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤32K
        theta (float): æ—‹è½¬é¢‘ç‡çš„åŸºæ•°ï¼Œé»˜è®¤1e6
        
    Returns:
        tuple: (freqs_cos, freqs_sin) - é¢„è®¡ç®—çš„ä½™å¼¦å’Œæ­£å¼¦é¢‘ç‡çŸ©é˜µ
    """
    # è®¡ç®—é¢‘ç‡åºåˆ—ï¼š1.0 / (theta^(2i/dim)) for i in [0, 2, 4, ..., dim-2]
    # è®­ç»ƒä½œç”¨ï¼šé¢„å®šä¹‰çš„é¢‘ç‡æ¨¡å¼ï¼Œä¸å‚ä¸è®­ç»ƒä½†å½±å“ä½ç½®ç¼–ç æ•ˆæœ
    # æ¨ç†ä½œç”¨ï¼šæä¾›ä½ç½®æ•æ„Ÿçš„ç‰¹å¾å˜æ¢ï¼Œä½¿æ¨¡å‹èƒ½åŒºåˆ†ä¸åŒä½ç½®çš„token
    # åªå–å¶æ•°ç´¢å¼•ï¼šRoPEç®—æ³•è¦æ±‚æˆå¯¹å¤„ç†ç›¸é‚»ç»´åº¦
    indices = torch.arange(0, dim, 2)[: (dim // 2)].float()  # [0, 2, 4, ..., dim-2]
    freqs = 1.0 / (theta ** (indices / dim))  # è®¡ç®—åŸºç¡€é¢‘ç‡
    
    # ç”Ÿæˆä½ç½®åºåˆ—ï¼š[0, 1, 2, ..., end-1]
    # è®­ç»ƒä½œç”¨ï¼šè¦†ç›–è®­ç»ƒåºåˆ—çš„æ‰€æœ‰å¯èƒ½ä½ç½®
    # æ¨ç†ä½œç”¨ï¼šæ”¯æŒä»»æ„é•¿åº¦åºåˆ—çš„ä½ç½®ç¼–ç ï¼Œå®ç°é•¿åº¦å¤–æ¨
    t = torch.arange(end, device=freqs.device)
    
    # è®¡ç®—å¤–ç§¯ï¼šæ¯ä¸ªä½ç½®ä¸æ¯ä¸ªé¢‘ç‡çš„ç»„åˆ
    # è®­ç»ƒä½œç”¨ï¼šä¸ºæ¯ä¸ªä½ç½®-é¢‘ç‡å¯¹ç”Ÿæˆå”¯ä¸€çš„ç›¸ä½
    # æ¨ç†ä½œç”¨ï¼šæä¾›è¿ç»­çš„ä½ç½®ç¼–ç ï¼Œä¿è¯ä½ç½®é—´çš„ç›¸å¯¹å…³ç³»
    # å½¢çŠ¶ï¼š(end, dim//2)
    freqs = torch.outer(t, freqs).float()
    
    # è®¡ç®—ä½™å¼¦å’Œæ­£å¼¦å€¼ï¼Œå¹¶å¤åˆ¶ä»¥åŒ¹é…å®Œæ•´ç»´åº¦
    # è®­ç»ƒä½œç”¨ï¼šæä¾›å¹³æ»‘çš„ä½ç½®ç¼–ç æ¢¯åº¦ï¼Œæœ‰åˆ©äºä½ç½®ä¿¡æ¯çš„å­¦ä¹ 
    # æ¨ç†ä½œç”¨ï¼šå®ç°é«˜æ•ˆçš„æ—‹è½¬å˜æ¢ï¼Œä¿æŒå‘é‡é•¿åº¦ä¸å˜
    # å¤åˆ¶ä¸€ä»½ï¼šå› ä¸ºRoPEéœ€è¦å¯¹ç›¸é‚»ç»´åº¦æˆå¯¹åº”ç”¨
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)  # å½¢çŠ¶ï¼š(end, dim)
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)  # å½¢çŠ¶ï¼š(end, dim)
    
    return freqs_cos, freqs_sin


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """
    å°†æ—‹è½¬ä½ç½®ç¼–ç (RoPE)åº”ç”¨åˆ°æŸ¥è¯¢ï¼ˆQï¼‰å’Œé”®ï¼ˆKï¼‰å‘é‡ä¸Š
    
    ğŸ¯ è®ºæ–‡å¯¹ç…§è§£æ - RoFormer: Enhanced Transformer with Rotary Position Embedding
    ===========================================================================
    
    ğŸ“š ç†è®ºèƒŒæ™¯ï¼š
    -----------
    RoPEæ˜¯ç”±è‹å‰‘æ—ç­‰äººåœ¨2021å¹´æå‡ºçš„ä½ç½®ç¼–ç æ–¹æ³•ï¼Œè®ºæ–‡æ ‡é¢˜ï¼š
    "RoFormer: Enhanced Transformer with Rotary Position Embedding"
    
    ğŸ” ä¸ä¼ ç»Ÿä½ç½®ç¼–ç çš„å¯¹æ¯”ï¼š
    ----------------------
    1. **ç»å¯¹ä½ç½®ç¼–ç  (Attention is All You Need)**ï¼š
       - åœ¨è¾“å…¥è¯åµŒå…¥æ—¶åŠ ä¸Šsin/cosä½ç½®ç¼–ç 
       - é—®é¢˜ï¼šä½ç½®ä¿¡æ¯ä¼šéšç€å±‚æ•°å¢åŠ è€Œè¡°å‡
    
    2. **ç›¸å¯¹ä½ç½®ç¼–ç  (Self-Attention with Relative Position)**ï¼š
       - åœ¨æ³¨æ„åŠ›è®¡ç®—æ—¶åŠ å…¥ç›¸å¯¹ä½ç½®åç½®
       - é—®é¢˜ï¼šé¢å¤–çš„å‚æ•°å¼€é”€å’Œè®¡ç®—å¤æ‚åº¦
       
    3. **æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)**ï¼šâ­ æœ¬å‡½æ•°å®ç°
       - é€šè¿‡æ—‹è½¬å˜æ¢å°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°Qå’ŒKä¸­
       - ä¼˜åŠ¿ï¼šåŒæ—¶å…·å¤‡ç»å¯¹å’Œç›¸å¯¹ä½ç½®æ„ŸçŸ¥ï¼Œä¸”æ— é¢å¤–å‚æ•°
    
    ğŸ§® æ•°å­¦åŸç†è¯¦è§£ï¼š
    ---------------
    è®ºæ–‡ä¸­çš„æ ¸å¿ƒå…¬å¼ï¼š
    
    å¯¹äºä½ç½®mçš„queryå‘é‡qå’Œä½ç½®nçš„keyå‘é‡kï¼š
    q_m' = R_Î˜,m * q_m
    k_n' = R_Î˜,n * k_n
    
    å…¶ä¸­R_Î˜,mæ˜¯æ—‹è½¬çŸ©é˜µï¼š
    R_Î˜,m = [cos(mÎ¸)  -sin(mÎ¸)]
            [sin(mÎ¸)   cos(mÎ¸)]
    
    å…³é”®æ´å¯Ÿï¼šq_m' Â· k_n' = q_m Â· R_Î˜,m-n Â· k_n
    è¿™æ„å‘³ç€æ³¨æ„åŠ›åˆ†æ•°åªä¾èµ–äºç›¸å¯¹ä½ç½®å·® (m-n)ï¼
    
    ğŸ’¡ å¤æ•°åŸŸè¡¨ç¤ºï¼š
    ------------
    è®ºæ–‡å°†å‘é‡å¯¹çœ‹ä½œå¤æ•°ï¼šq = [qâ‚€, qâ‚] â†’ qâ‚€ + i*qâ‚
    æ—‹è½¬æ“ä½œï¼šq' = q * e^(i*m*Î¸) = q * (cos(mÎ¸) + i*sin(mÎ¸))
    
    ğŸ”§ å®æ•°åŸŸå®ç°ï¼š
    ------------
    æœ¬å‡½æ•°å°†å¤æ•°æ—‹è½¬è½¬æ¢ä¸ºå®æ•°è®¡ç®—ï¼š
    å¯¹äºå‘é‡ [xâ‚€, xâ‚, xâ‚‚, xâ‚ƒ, ...]ï¼Œæ¯ç›¸é‚»ä¸¤ä¸ªå…ƒç´ ç»„æˆä¸€ä¸ªå¤æ•°
    [xâ‚€ + i*xâ‚, xâ‚‚ + i*xâ‚ƒ, ...] è¿›è¡Œæ—‹è½¬
    
    Args:
        q (torch.Tensor): æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, num_heads, head_dim)
        k (torch.Tensor): é”®å‘é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, num_heads, head_dim)
        cos (torch.Tensor): é¢„è®¡ç®—çš„ä½™å¼¦é¢‘ç‡çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (seq_len, head_dim//2)
        sin (torch.Tensor): é¢„è®¡ç®—çš„æ­£å¼¦é¢‘ç‡çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (seq_len, head_dim//2)
        position_ids (torch.Tensor, optional): ä½ç½®IDï¼Œä¿ç•™å‚æ•°ä½†æ­¤å®ç°ä¸­æœªä½¿ç”¨
        unsqueeze_dim (int): åœ¨å“ªä¸ªç»´åº¦æ·»åŠ ç»´åº¦ä»¥è¿›è¡Œå¹¿æ’­ï¼Œé»˜è®¤ä¸º1(headç»´åº¦)
        
    Returns:
        tuple: (q_embed, k_embed) - åº”ç”¨äº†RoPEä½ç½®ç¼–ç çš„æŸ¥è¯¢å’Œé”®å‘é‡
        
    ğŸ¯ ä¸è®ºæ–‡å…¬å¼çš„å¯¹åº”å…³ç³»ï¼š
    --------------------
    è®ºæ–‡å…¬å¼: f_q(x_m, m) = R_Î˜,m W_q x_m
    ä»£ç å®ç°: q_embed = (q * cos) + (rotate_half(q) * sin)
    
    å…¶ä¸­ï¼š
    - q = W_q x_m (çº¿æ€§æŠ•å½±åçš„æŸ¥è¯¢å‘é‡)
    - cos, sin = cos(mÎ¸), sin(mÎ¸) (ä½ç½®mçš„æ—‹è½¬è§’åº¦)
    - rotate_half å®ç°å¤æ•°æ—‹è½¬çš„è™šéƒ¨æ“ä½œ
    """
    def rotate_half(x):
        """
        å®ç°å¤æ•°æ—‹è½¬çš„å…³é”®å‡½æ•° - å¯¹åº”è®ºæ–‡ä¸­çš„è™šéƒ¨å¤„ç†
        
        ğŸ§® æ•°å­¦åŸç†ï¼š
        -----------
        å¯¹äºå¤æ•° z = a + biï¼Œæ—‹è½¬ e^(iÎ¸) åï¼š
        z' = z * e^(iÎ¸) = (a + bi) * (cos(Î¸) + i*sin(Î¸))
           = (a*cos(Î¸) - b*sin(Î¸)) + i*(a*sin(Î¸) + b*cos(Î¸))
        
        åœ¨å®æ•°åŸŸä¸­ï¼Œæˆ‘ä»¬å°† [a, b] çœ‹ä½œå¤æ•° a + biï¼š
        - å®éƒ¨ï¼ša*cos(Î¸) - b*sin(Î¸) = [a, b] * cos(Î¸) + [-b, a] * sin(Î¸)
        - rotate_half([a, b]) = [-b, a] å¯¹åº”å¤æ•°ä¹˜æ³•ä¸­çš„ i*z æ“ä½œ
        
        ğŸ“Š å…·ä½“å˜æ¢ï¼š
        -----------
        è¾“å…¥å‘é‡ï¼š[xâ‚€, xâ‚, xâ‚‚, xâ‚ƒ, xâ‚„, xâ‚…] (head_dim=6)
        åˆ†ç»„å¤æ•°ï¼š[(xâ‚€, xâ‚), (xâ‚‚, xâ‚ƒ), (xâ‚„, xâ‚…)]
        rotate_halfï¼š[xâ‚, xâ‚€, xâ‚ƒ, xâ‚‚, xâ‚…, xâ‚„] â†’ [-xâ‚, xâ‚€, -xâ‚ƒ, xâ‚‚, -xâ‚…, xâ‚„]
        ç­‰ä»·äºï¼š[(-xâ‚, xâ‚€), (-xâ‚ƒ, xâ‚‚), (-xâ‚…, xâ‚„)] = i * åŸå¤æ•°
        
        Args:
            x (torch.Tensor): è¾“å…¥å‘é‡ï¼Œæœ€åä¸€ç»´åº”ä¸ºå¶æ•°
            
        Returns:
            torch.Tensor: æ—‹è½¬åçš„å‘é‡ï¼Œå¯¹åº”å¤æ•°åŸŸä¸­çš„ i*x æ“ä½œ
        """
        # è·å–å‘é‡çš„ä¸­ç‚¹ä½ç½® - å¯¹åº”å¤æ•°çš„å®éƒ¨å’Œè™šéƒ¨åˆ†ç•Œç‚¹
        # è®ºæ–‡ä¸­æ¯ä¸¤ä¸ªç›¸é‚»å…ƒç´ ç»„æˆä¸€ä¸ªå¤æ•°è¿›è¡Œæ—‹è½¬
        # è®­ç»ƒä½œç”¨ï¼šå®ç°å¯å¾®çš„å¤æ•°æ—‹è½¬æ“ä½œï¼Œä¿æŒæ¢¯åº¦æµçš„è¿ç»­æ€§
        # æ¨ç†ä½œç”¨ï¼šé«˜æ•ˆè®¡ç®—ä½ç½®ç¼–ç ï¼Œæ— é¢å¤–å‚æ•°å’Œå†…å­˜å¼€é”€
        mid = x.shape[-1] // 2
        
        # åˆ†å‰²ä¸ºå‰åŠéƒ¨åˆ†(å®éƒ¨)å’ŒååŠéƒ¨åˆ†(è™šéƒ¨)
        # è®ºæ–‡å¯¹åº”ï¼šå°†head_dimç»´åº¦åˆ†ä¸ºhead_dim/2ä¸ªå¤æ•°
        x1, x2 = x[..., :mid], x[..., mid:]  # x1å¯¹åº”å®éƒ¨ï¼Œx2å¯¹åº”è™šéƒ¨
        
        # å®ç° i*z = i*(a + bi) = -b + ai çš„å˜æ¢
        # å¯¹åº”è®ºæ–‡ä¸­å¤æ•°æ—‹è½¬çš„è™šéƒ¨è®¡ç®—
        # è®­ç»ƒä½œç”¨ï¼šä¿æŒæ—‹è½¬ä¸å˜æ€§ï¼Œä½¿æ¨¡å‹å­¦ä¹ åˆ°ä½ç½®æ— å…³çš„è¡¨ç¤º
        # æ¨ç†ä½œç”¨ï¼šå‡†ç¡®å®ç°è®ºæ–‡ä¸­çš„æ•°å­¦å…¬å¼ï¼Œç¡®ä¿ä½ç½®ç¼–ç çš„æ­£ç¡®æ€§
        return torch.cat((-x2, x1), dim=-1)  # [-x2, x1] = [-è™šéƒ¨, å®éƒ¨] = i*å¤æ•°

    # === RoPEæ—‹è½¬å˜æ¢çš„æ ¸å¿ƒå®ç° ===
    # ğŸ§® è®ºæ–‡å…¬å¼å®ç°ï¼šq'_m = q_m * cos(mÎ¸) + rotate_half(q_m) * sin(mÎ¸)
    # =================================================================
    # 
    # ğŸ“– è®ºæ–‡å¯¹åº”å…³ç³»ï¼š
    # ---------------
    # è®ºæ–‡å…¬å¼ï¼šf_q(x_m, m) = R_Î˜,m W_q x_m
    # å…¶ä¸­ R_Î˜,m æ˜¯æ—‹è½¬çŸ©é˜µï¼Œåœ¨å¤æ•°åŸŸè¡¨ç¤ºä¸ºï¼še^(i*m*Î¸)
    # 
    # å®æ•°åŸŸå±•å¼€ï¼š
    # z * e^(iÎ¸) = (a + bi) * (cos(Î¸) + i*sin(Î¸))
    #            = (a*cos(Î¸) - b*sin(Î¸)) + i*(a*sin(Î¸) + b*cos(Î¸))
    # 
    # å¯¹åº”åˆ°ä»£ç ï¼š
    # q_embed = q * cos + rotate_half(q) * sin
    # å…¶ä¸­ rotate_half(q) å®ç°äº†å¤æ•°ä¹˜æ³•ä¸­çš„ i*q æ“ä½œ
    
    # ğŸ”§ ç»´åº¦æ‰©å±•å¤„ç†ï¼š
    # ---------------
    # coså’Œsinçš„å½¢çŠ¶ï¼š[seq_len, head_dim//2]
    # qå’Œkçš„å½¢çŠ¶ï¼š[batch, seq_len, num_heads, head_dim]
    # éœ€è¦åœ¨headç»´åº¦ä¸Šæ‰©å±•ä»¥æ”¯æŒå¹¿æ’­è¿ç®—
    # 
    # ğŸ“Š æ‰©å±•ç¤ºä¾‹ï¼š
    # -----------
    # åŸå§‹ï¼šcos [512, 32]ï¼Œsin [512, 32] (å‡è®¾head_dim=64)
    # æ‰©å±•ï¼šcos [512, 1, 32]ï¼Œsin [512, 1, 32] (åœ¨ç»´åº¦1æ’å…¥)
    # å¹¿æ’­ï¼šcos [512, 8, 32]ï¼Œsin [512, 8, 32] (è‡ªåŠ¨æ‰©å±•åˆ°8ä¸ªå¤´)
    # 
    # è®­ç»ƒä½œç”¨ï¼šç¡®ä¿æ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½è·å¾—ç›¸åŒçš„ä½ç½®ç¼–ç ï¼Œä¿æŒä½ç½®ä¿¡æ¯çš„ä¸€è‡´æ€§
    # æ¨ç†ä½œç”¨ï¼šé«˜æ•ˆçš„å‘é‡åŒ–è®¡ç®—ï¼Œé¿å…å¾ªç¯æ“ä½œæå‡æ¨ç†é€Ÿåº¦
    cos_expanded = cos.unsqueeze(unsqueeze_dim)  # åœ¨headç»´åº¦æ’å…¥æ–°ç»´åº¦
    sin_expanded = sin.unsqueeze(unsqueeze_dim)
    
    # === åº”ç”¨RoPEåˆ°æŸ¥è¯¢å‘é‡ ===
    # ğŸ¯ è®ºæ–‡æ ¸å¿ƒï¼šä¸ºæŸ¥è¯¢å‘é‡æ³¨å…¥ç»å¯¹ä½ç½®ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒç›¸å¯¹ä½ç½®æ„ŸçŸ¥
    # =============================================================
    # 
    # ğŸ“š ç†è®ºæ„ä¹‰ï¼š
    # -----------
    # 1. **ç»å¯¹ä½ç½®ç¼–ç **ï¼šæ¯ä¸ªä½ç½®mçš„æŸ¥è¯¢å‘é‡éƒ½ä¼šä¹˜ä»¥ e^(i*m*Î¸)
    # 2. **ç›¸å¯¹ä½ç½®æ„ŸçŸ¥**ï¼šq_m Â· k_n = q_m Â· R_{m-n} Â· k_nï¼Œåªä¾èµ–ä½ç½®å·®
    # 3. **æ—‹è½¬ä¸å˜æ€§**ï¼šæ•´ä¸ªåºåˆ—åŒæ—¶æ—‹è½¬Î¸è§’åº¦ï¼Œç›¸å¯¹å…³ç³»ä¿æŒä¸å˜
    # 
    # ğŸ”¢ æ•°å­¦éªŒè¯ï¼š
    # -----------
    # åŸå§‹æ³¨æ„åŠ›ï¼šscore = q_m Â· k_n
    # RoPEåï¼šscore' = (R_m q_m) Â· (R_n k_n) = q_m Â· R_m^T R_n Â· k_n = q_m Â· R_{n-m} Â· k_n
    # åªä¾èµ–äºç›¸å¯¹ä½ç½®å·® (n-m)ï¼Œå®ç°äº†ç›¸å¯¹ä½ç½®ç¼–ç çš„æ•ˆæœï¼
    # 
    # ğŸš€ å®ç°ç»†èŠ‚ï¼š
    # -----------
    # ç¬¬ä¸€é¡¹ï¼šq * cos_expanded
    # - å¯¹åº”å¤æ•°ä¹˜æ³•çš„å®éƒ¨ï¼šReal(q) * cos(mÎ¸)
    # ç¬¬äºŒé¡¹ï¼šrotate_half(q) * sin_expanded  
    # - å¯¹åº”å¤æ•°ä¹˜æ³•çš„è™šéƒ¨ï¼šImag(q) * sin(mÎ¸)
    # 
    # è®­ç»ƒä½œç”¨ï¼šä¸ºæŸ¥è¯¢å‘é‡æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œä½¿æ³¨æ„åŠ›æƒé‡å…·æœ‰ä½ç½®æ„ŸçŸ¥èƒ½åŠ›
    # æ¨ç†ä½œç”¨ï¼šç¡®ä¿ç”Ÿæˆæ—¶èƒ½æ­£ç¡®ç†è§£tokençš„ç»å¯¹å’Œç›¸å¯¹ä½ç½®å…³ç³»
    q_embed = (q * cos_expanded) + (rotate_half(q) * sin_expanded)
    
    # === åº”ç”¨RoPEåˆ°é”®å‘é‡ ===
    # ğŸ¯ ä¸æŸ¥è¯¢å‘é‡é…å¯¹ï¼Œå½¢æˆå®Œæ•´çš„ä½ç½®æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶
    # ================================================
    # 
    # ğŸ’¡ å…³é”®æ´å¯Ÿï¼š
    # -----------
    # åªæœ‰åŒæ—¶å¯¹qå’Œkåº”ç”¨RoPEï¼Œæ‰èƒ½å®ç°è®ºæ–‡ä¸­çš„ç›¸å¯¹ä½ç½®æ•ˆæœï¼š
    # - å¦‚æœåªå¯¹qæˆ–åªå¯¹kåº”ç”¨ï¼šæ— æ³•å®ç°ç›¸å¯¹ä½ç½®ç¼–ç 
    # - åŒæ—¶åº”ç”¨ï¼šq_m Â· k_n â†’ (R_m q_m) Â· (R_n k_n) = q_m Â· R_{n-m} Â· k_n
    # 
    # ğŸ”„ å¯¹ç§°æ€§åŸç†ï¼š
    # -------------
    # æŸ¥è¯¢å‘é‡åœ¨ä½ç½®mæ—‹è½¬è§’åº¦ m*Î¸
    # é”®å‘é‡åœ¨ä½ç½®næ—‹è½¬è§’åº¦ n*Î¸  
    # æ³¨æ„åŠ›è®¡ç®—æ—¶çš„ç›¸å¯¹è§’åº¦ï¼š(m-n)*Î¸ï¼Œåªä¾èµ–ä½ç½®å·®ï¼
    # 
    # ğŸ“ˆ æ€§èƒ½ä¼˜åŠ¿ï¼š
    # -----------
    # 1. **æ— é¢å¤–å‚æ•°**ï¼šä¸åƒä¼ ç»Ÿç›¸å¯¹ä½ç½®ç¼–ç éœ€è¦å­¦ä¹ ä½ç½®åµŒå…¥
    # 2. **çº¿æ€§å¤æ‚åº¦**ï¼šä½ç½®ç¼–ç åº”ç”¨çš„å¤æ‚åº¦æ˜¯O(d)ï¼Œè€Œä¸æ˜¯O(nÂ²)
    # 3. **å¤–æ¨èƒ½åŠ›**ï¼šå¯ä»¥å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„æ›´é•¿åºåˆ—
    # 
    # è®­ç»ƒä½œç”¨ï¼šä¸ºé”®å‘é‡æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œä¸æŸ¥è¯¢å‘é‡çš„ä½ç½®ç¼–ç å½¢æˆé…å¯¹
    # æ¨ç†ä½œç”¨ï¼šç¡®ä¿æ³¨æ„åŠ›è®¡ç®—ä¸­ä½ç½®å…³ç³»çš„æ•°å­¦æ­£ç¡®æ€§å’Œä¸€è‡´æ€§
    k_embed = (k * cos_expanded) + (rotate_half(k) * sin_expanded)
    
    return q_embed, k_embed


def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    é‡å¤é”®å€¼ï¼ˆKey-Valueï¼‰å¼ é‡ä»¥æ”¯æŒåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGrouped Query Attention, GQAï¼‰
    
    åœ¨GQAä¸­ï¼ŒæŸ¥è¯¢å¤´çš„æ•°é‡é€šå¸¸å¤§äºé”®å€¼å¤´çš„æ•°é‡ï¼Œéœ€è¦å°†é”®å€¼å¼ é‡é‡å¤ä»¥åŒ¹é…æŸ¥è¯¢å¤´çš„æ•°é‡ã€‚
    è¿™æ ·å¯ä»¥å‡å°‘KVç¼“å­˜çš„å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚
    
    Args:
        x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (bs, slen, num_key_value_heads, head_dim)
        n_rep (int): é‡å¤æ¬¡æ•°ï¼Œé€šå¸¸ä¸º num_attention_heads // num_key_value_heads
        
    Returns:
        torch.Tensor: é‡å¤åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º (bs, slen, num_attention_heads, head_dim)
        
    Note:
        ç­‰ä»·äº torch.repeat_interleave(x, dim=2, repeats=n_rep)ï¼Œä½†å®ç°æ›´é«˜æ•ˆ
    """
    # è·å–è¾“å…¥å¼ é‡çš„ç»´åº¦ä¿¡æ¯
    # è®­ç»ƒä½œç”¨ï¼šç¡®ä¿å¼ é‡å½¢çŠ¶çš„æ­£ç¡®æ€§ï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…é”™è¯¯
    # æ¨ç†ä½œç”¨ï¼šä¸ºåç»­çš„å¼ é‡æ“ä½œæä¾›å‡†ç¡®çš„å½¢çŠ¶ä¿¡æ¯
    bs, slen, num_key_value_heads, head_dim = x.shape
    
    # å¦‚æœä¸éœ€è¦é‡å¤ï¼Œç›´æ¥è¿”å›åŸå¼ é‡
    # è®­ç»ƒä½œç”¨ï¼šåœ¨æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ï¼ˆéGQAï¼‰ä¸­é¿å…ä¸å¿…è¦çš„è®¡ç®—
    # æ¨ç†ä½œç”¨ï¼šä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œå‡å°‘å†…å­˜æ‹·è´
    if n_rep == 1:
        return x
    
    # é«˜æ•ˆçš„å¼ é‡é‡å¤å®ç°
    # æ­¥éª¤1ï¼šåœ¨ç¬¬4ä¸ªç»´åº¦ï¼ˆæ–°å¢ç»´åº¦ï¼‰æ’å…¥ä¸€ä¸ªç»´åº¦
    # è®­ç»ƒä½œç”¨ï¼šå‡†å¤‡å¼ é‡ç»“æ„ä»¥è¿›è¡Œé‡å¤æ“ä½œï¼Œä¿æŒæ¢¯åº¦ä¼ æ’­çš„æ­£ç¡®æ€§
    # æ¨ç†ä½œç”¨ï¼šåˆ›å»ºé‡å¤æ¨¡æ¿ï¼Œä¸ºåç»­expandæ“ä½œåšå‡†å¤‡
    x_expanded = x[:, :, :, None, :]  # å½¢çŠ¶ï¼š(bs, slen, num_kv_heads, 1, head_dim)
    
    # æ­¥éª¤2ï¼šåœ¨æ–°å¢çš„ç»´åº¦ä¸Šæ‰©å±•n_repæ¬¡
    # è®­ç»ƒä½œç”¨ï¼šå®ç°é”®å€¼å¤´çš„é€»è¾‘é‡å¤ï¼Œä½¿GQAèƒ½å¤Ÿä¸å¤šå¤´æŸ¥è¯¢åŒ¹é…
    # æ¨ç†ä½œç”¨ï¼šé«˜æ•ˆçš„å†…å­˜æ‰©å±•ï¼Œæ¯”ç›´æ¥å¤åˆ¶æ›´èŠ‚çœå†…å­˜
    x_repeated = x_expanded.expand(bs, slen, num_key_value_heads, n_rep, head_dim)
    
    # æ­¥éª¤3ï¼šé‡å¡‘å¼ é‡ä»¥åˆå¹¶é‡å¤çš„ç»´åº¦
    # è®­ç»ƒä½œç”¨ï¼šå°†é‡å¤çš„é”®å€¼å¤´å±•å¹³ä¸ºç‹¬ç«‹çš„å¤´ï¼ŒåŒ¹é…æŸ¥è¯¢å¤´çš„æ•°é‡
    # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆæœ€ç»ˆå½¢çŠ¶ï¼Œä½¿åç»­æ³¨æ„åŠ›è®¡ç®—èƒ½å¤Ÿæ­£ç¡®å¯¹é½
    result = x_repeated.reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    
    return result


class Attention(nn.Module):
    """
    å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attentionï¼‰
    =======================================
    
    é¡¹ç›®ä¸­çš„ä½œç”¨ï¼š
    - ä½œä¸ºMiniMindæ¨¡å‹çš„æ ¸å¿ƒè®¡ç®—ç»„ä»¶ï¼Œå®ç°åºåˆ—å†…ä¿¡æ¯äº¤äº’
    - è´Ÿè´£æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œä¸Šä¸‹æ–‡è¯­ä¹‰ç†è§£
    - æ”¯æŒé«˜æ•ˆæ¨ç†çš„KVç¼“å­˜æœºåˆ¶ï¼Œå®ç°æµå¼æ–‡æœ¬ç”Ÿæˆ
    
    å¤§æ¨¡å‹æ¡†æ¶ä¸­çš„ä½œç”¨ï¼š
    - å®ç°ç°ä»£Transformeræ¶æ„çš„æ³¨æ„åŠ›æœºåˆ¶æ ‡å‡†
    - é›†æˆåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰æŠ€æœ¯ï¼Œå¹³è¡¡æ€§èƒ½å’Œå†…å­˜æ•ˆç‡
    - æ”¯æŒFlash AttentionåŠ é€Ÿï¼Œé€‚åº”å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒå’Œæ¨ç†
    - æä¾›å®Œæ•´çš„ä½ç½®ç¼–ç é›†æˆï¼ˆRoPEï¼‰ï¼Œå¢å¼ºä½ç½®æ„ŸçŸ¥èƒ½åŠ›
    
    æŠ€æœ¯åˆ›æ–°ç‚¹ï¼š
    1. åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ï¼š
       - å‡å°‘é”®å€¼å¤´æ•°é‡ï¼Œé™ä½KVç¼“å­˜å†…å­˜å ç”¨
       - åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æå‡æ¨ç†æ•ˆç‡
       
    2. Flash Attentionæ”¯æŒï¼š
       - å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—å®ç°
       - æ”¯æŒé•¿åºåˆ—è®­ç»ƒå’Œæ¨ç†
       
    3. æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰é›†æˆï¼š
       - ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œå…·å¤‡æ›´å¥½çš„å¤–æ¨èƒ½åŠ›
       - æ”¯æŒæ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—å¤„ç†
    
    4. KVç¼“å­˜ä¼˜åŒ–ï¼š
       - å¢é‡è§£ç æœºåˆ¶ï¼Œé¿å…é‡å¤è®¡ç®—
       - æ”¯æŒæµå¼ç”Ÿæˆå’Œå®æ—¶å¯¹è¯
    
    è®¡ç®—æµç¨‹ï¼š
    Q, K, V â† Linear(X)  # çº¿æ€§æŠ•å½±
    Q, K â† RoPE(Q, K)    # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
    K, V â† Concat(Past_KV, K, V)  # KVç¼“å­˜æ‹¼æ¥
    Attn â† Attention(Q, K, V)     # æ³¨æ„åŠ›è®¡ç®—
    Output â† Linear(Attn)         # è¾“å‡ºæŠ•å½±
    
    é€‚ç”¨åœºæ™¯ï¼š
    - å› æœè¯­è¨€æ¨¡å‹çš„è‡ªå›å½’ç”Ÿæˆ
    - é•¿æ–‡æœ¬ç†è§£å’Œç”Ÿæˆä»»åŠ¡
    - å¯¹è¯ç³»ç»Ÿå’Œæ–‡æœ¬ç»­å†™
    - ä»£ç ç”Ÿæˆå’Œé—®ç­”ä»»åŠ¡
    """
    
    def __init__(self, args: MiniMindConfig):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›å±‚
        
        Args:
            args (MiniMindConfig): æ¨¡å‹é…ç½®å‚æ•°
        """
        super().__init__()
        
        # è®¾ç½®é”®å€¼å¤´æ•°é‡ï¼šå¦‚æœæœªæŒ‡å®šåˆ™ä¸æ³¨æ„åŠ›å¤´æ•°é‡ç›¸åŒï¼ˆæ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ï¼‰
        # è®­ç»ƒä½œç”¨ï¼šç¡®å®šKVç¼“å­˜çš„å†…å­˜éœ€æ±‚å’Œè®¡ç®—å¤æ‚åº¦
        # æ¨ç†ä½œç”¨ï¼šå½±å“æ¨ç†æ—¶çš„å†…å­˜å ç”¨å’Œè®¡ç®—æ•ˆç‡
        self.num_key_value_heads = (args.num_attention_heads 
                                   if args.num_key_value_heads is None 
                                   else args.num_key_value_heads)
        
        # ç¡®ä¿æ³¨æ„åŠ›å¤´æ•°é‡èƒ½è¢«é”®å€¼å¤´æ•°é‡æ•´é™¤ï¼ˆGQAçš„æ•°å­¦è¦æ±‚ï¼‰
        # è®­ç»ƒä½œç”¨ï¼šéªŒè¯é…ç½®çš„æ­£ç¡®æ€§ï¼Œé¿å…è®­ç»ƒæ—¶çš„ç»´åº¦é”™è¯¯
        # æ¨ç†ä½œç”¨ï¼šç¡®ä¿repeat_kvå‡½æ•°èƒ½å¤Ÿæ­£ç¡®å·¥ä½œ
        assert args.num_attention_heads % self.num_key_value_heads == 0
        
        # ç¼“å­˜å…³é”®ç»´åº¦ä¿¡æ¯ä»¥ä¼˜åŒ–åç»­è®¡ç®—
        self.n_local_heads = args.num_attention_heads      # æŸ¥è¯¢å¤´æ•°é‡
        self.n_local_kv_heads = self.num_key_value_heads   # é”®å€¼å¤´æ•°é‡
        
        # è®¡ç®—æ¯ä¸ªKVå¤´å¯¹åº”çš„Qå¤´æ•°é‡ï¼ˆGQAçš„æ ¸å¿ƒå‚æ•°ï¼‰
        # è®­ç»ƒä½œç”¨ï¼šå†³å®šé”®å€¼å¼ é‡çš„é‡å¤å€æ•°ï¼Œå½±å“å†…å­˜å’Œè®¡ç®—æ•ˆç‡
        # æ¨ç†ä½œç”¨ï¼šæ§åˆ¶KVç¼“å­˜çš„å¤ç”¨ç¨‹åº¦ï¼Œé™ä½å†…å­˜éœ€æ±‚
        self.n_rep = self.n_local_heads // self.n_local_kv_heads
        
        # è®¡ç®—æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
        # ğŸ¯ æ ¸å¿ƒåŸç†è§£é‡Šï¼šä¸ºä»€ä¹ˆ head_dim = hidden_size // num_attention_headsï¼Ÿ
        # =====================================================================
        # 
        # ğŸ“š ç†è®ºåŸºç¡€ - å¤šå¤´æ³¨æ„åŠ›çš„æ•°å­¦åŸç†ï¼š
        # ----------------------------------
        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¤§çš„æ³¨æ„åŠ›è®¡ç®—åˆ†è§£ä¸ºå¤šä¸ªå°çš„ã€å¹¶è¡Œçš„æ³¨æ„åŠ›è®¡ç®—ã€‚
        # æ¯ä¸ª"å¤´"(head)ä¸“æ³¨äºæ•æ‰ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»å’Œæ¨¡å¼ã€‚
        #
        # ğŸ”¢ ç»´åº¦åˆ†è§£çš„æ•°å­¦é€»è¾‘ï¼š
        # ----------------------
        # 1. æ€»çš„ç‰¹å¾ç»´åº¦ä¿æŒä¸å˜ï¼š
        #    - è¾“å…¥ç»´åº¦ï¼šhidden_size (ä¾‹å¦‚ï¼š512)
        #    - è¾“å‡ºç»´åº¦ï¼šä¹Ÿå¿…é¡»æ˜¯ hidden_size (ä¾‹å¦‚ï¼š512)
        #    
        # 2. å¤šå¤´å¹¶è¡Œå¤„ç†ï¼š
        #    - å‡è®¾æœ‰ 8 ä¸ªæ³¨æ„åŠ›å¤´
        #    - æ¯ä¸ªå¤´å¤„ç†éƒ¨åˆ†ç‰¹å¾ï¼š512 Ã· 8 = 64 ç»´
        #    - 8ä¸ªå¤´å„è‡ªå¤„ç†64ç»´ï¼Œæœ€åæ‹¼æ¥ï¼š8 Ã— 64 = 512 ç»´
        #
        # ğŸ¯ å…·ä½“è®¡ç®—ç¤ºä¾‹ï¼š
        # ---------------
        # å‡è®¾ hidden_size=512, num_attention_heads=8ï¼š
        # - head_dim = 512 // 8 = 64
        # - QçŸ©é˜µå½¢çŠ¶ï¼š[512, 8Ã—64] = [512, 512]
        # - æ¯ä¸ªå¤´çš„Qï¼š[batch, seq_len, 64]
        # - 8ä¸ªå¤´æ‹¼æ¥åï¼š[batch, seq_len, 8Ã—64] = [batch, seq_len, 512]
        #
        # ğŸ’¡ ä¸ºä»€ä¹ˆè¦è¿™æ ·åˆ†è§£ï¼Ÿ
        # -------------------
        # 1. **å¹¶è¡Œè®¡ç®—**ï¼š8ä¸ªå°æ³¨æ„åŠ›å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œæé«˜æ•ˆç‡
        # 2. **ç‰¹å¾å¤šæ ·æ€§**ï¼šæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼
        #    - Head1å¯èƒ½å…³æ³¨å¥æ³•å…³ç³»
        #    - Head2å¯èƒ½å…³æ³¨è¯­ä¹‰å…³ç³»  
        #    - Head3å¯èƒ½å…³æ³¨é•¿è·ç¦»ä¾èµ–...
        # 3. **è®¡ç®—æ•ˆç‡**ï¼š8ä¸ª64ç»´çš„æ³¨æ„åŠ›æ¯”1ä¸ª512ç»´çš„æ³¨æ„åŠ›æ›´é«˜æ•ˆ
        #    - æ³¨æ„åŠ›å¤æ‚åº¦ï¼šO(seq_lenÂ² Ã— head_dim)
        #    - å¤šå¤´ï¼š8 Ã— O(seq_lenÂ² Ã— 64) vs å•å¤´ï¼šO(seq_lenÂ² Ã— 512)
        #    - å¹¶è¡ŒåŒ–åï¼Œå¤šå¤´å®é™…æ›´å¿«
        #
        # ğŸ” æ•°å­¦éªŒè¯ - ç»´åº¦ä¸€è‡´æ€§ï¼š
        # -------------------------
        # è¾“å…¥ï¼šx [batch, seq_len, hidden_size]
        #   â†“ QæŠ•å½±
        # Qï¼š[batch, seq_len, num_heads Ã— head_dim] = [batch, seq_len, hidden_size]
        #   â†“ é‡å¡‘ä¸ºå¤šå¤´
        # Qï¼š[batch, seq_len, num_heads, head_dim]
        #   â†“ è½¬ç½®
        # Qï¼š[batch, num_heads, seq_len, head_dim]
        #   â†“ æ³¨æ„åŠ›è®¡ç®—
        # outputï¼š[batch, num_heads, seq_len, head_dim]
        #   â†“ è½¬ç½®å›æ¥
        # outputï¼š[batch, seq_len, num_heads, head_dim]
        #   â†“ é‡å¡‘åˆå¹¶
        # outputï¼š[batch, seq_len, num_heads Ã— head_dim] = [batch, seq_len, hidden_size]
        #
        # âœ… ç»´åº¦å®Œç¾ä¿æŒï¼šè¾“å…¥512ç»´ â†’ è¾“å‡º512ç»´
        #
        # ğŸš€ å·¥ç¨‹å®ç°ä¼˜åŠ¿ï¼š
        # ---------------
        # 1. **å†…å­˜å¸ƒå±€ä¼˜åŒ–**ï¼šè¿ç»­çš„å†…å­˜è®¿é—®æ¨¡å¼
        # 2. **GPUå¹¶è¡Œå‹å¥½**ï¼šå¯ä»¥å……åˆ†åˆ©ç”¨GPUçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›
        # 3. **æ•°å€¼ç¨³å®šæ€§**ï¼šè¾ƒå°çš„head_dimæœ‰åŠ©äºæ•°å€¼ç¨³å®š
        # 4. **æ¢¯åº¦æµåŠ¨**ï¼šå¤šä¸ªå°çš„attentionæœ‰åŠ©äºæ¢¯åº¦ä¼ æ’­
        #
        # è®­ç»ƒä½œç”¨ï¼šç¡®å®šæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„è¡¨è¾¾èƒ½åŠ›å’Œå‚æ•°é‡åˆ†é…
        # æ¨ç†ä½œç”¨ï¼šå½±å“æ¨ç†æ—¶çš„è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚
        self.head_dim = args.hidden_size // args.num_attention_heads
        
        # === QKVæŠ•å½±çŸ©é˜µå®šä¹‰ ===
        # è¿™é‡Œå®šä¹‰äº†æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæƒé‡çŸ©é˜µï¼Œæ¯ä¸ªçŸ©é˜µéƒ½æ˜¯å¯å­¦ä¹ çš„å‚æ•°
        
        # ğŸ” æŸ¥è¯¢æŠ•å½±çŸ©é˜µ (Query Projection Matrix)
        # âš ï¸ é‡è¦æ¦‚å¿µè§£é‡Šï¼šself.q_proj å°±æ˜¯ä¼ ç»Ÿ Attention è®ºæ–‡ä¸­çš„ W_Q æƒé‡çŸ©é˜µï¼
        # 
        # ğŸ“š ç†è®ºå¯¹åº”å…³ç³»ï¼š
        # - è®ºæ–‡ä¸­: Q = X @ W_Q (Xæ˜¯è¾“å…¥ï¼ŒW_Qæ˜¯æŸ¥è¯¢æƒé‡çŸ©é˜µ)
        # - ä»£ç ä¸­: xq = self.q_proj(x) (ç­‰ä»·äº x @ W_Q + bï¼Œè¿™é‡Œbias=False)
        # 
        # ğŸ” çŸ©é˜µç»´åº¦åˆ†æï¼š
        # - W_Q å½¢çŠ¶: [hidden_size, num_attention_heads * head_dim]
        #   å…·ä½“æ•°å€¼: [512, 8 * 64] = [512, 512] â† è¿™æ‰æ˜¯çœŸæ­£çš„çŸ©é˜µå½¢çŠ¶ï¼
        # - è¾“å…¥ x å½¢çŠ¶: [batch_size, seq_len, hidden_size] 
        #   å…·ä½“ä¾‹å­: [4, 128, 512] â† hidden_size åªæ˜¯æœ€åä¸€ä¸ªç»´åº¦
        # - è¾“å‡º xq å½¢çŠ¶: [batch_size, seq_len, num_attention_heads * head_dim]
        #   å…·ä½“ä¾‹å­: [4, 128, 512] â† ç»è¿‡çº¿æ€§å˜æ¢åçš„å½¢çŠ¶
        # 
        # ğŸ’¡ ä¸ºä»€ä¹ˆå«"æŠ•å½±"ï¼Ÿ
        # - å°† hidden_size=512 ç»´åº¦çš„å‘é‡"æŠ•å½±"åˆ° (8*64)=512 ç»´åº¦ç©ºé—´
        # - æœ¬è´¨ä¸Šå°±æ˜¯çŸ©é˜µä¹˜æ³•: x @ W_Qï¼Œæ˜¯çº¿æ€§å˜æ¢çš„å‡ ä½•è§£é‡Š
        # - æ•°å­¦è¡¨ç¤º: [batch, seq, 512] @ [512, 512] = [batch, seq, 512]
        # 
        # ğŸ¯ nn.Linear çš„å†…éƒ¨å®ç°ï¼š
        # - nn.Linear(512, 512) å†…éƒ¨æœ‰æƒé‡çŸ©é˜µ self.weight [512, 512]
        # - forwardæ—¶æ‰§è¡Œ: F.linear(input, self.weight, self.bias) = input @ self.weight.T + bias
        # - æ‰€ä»¥ self.q_proj.weight å°±æ˜¯ä¼ ç»Ÿçš„ W_Q^T (è½¬ç½®å½¢å¼)
        # 
        # ğŸ” å…³é”®ç†è§£ï¼šhidden_size=512 æ˜¯"ç‰¹å¾ç»´åº¦"ï¼Œä¸æ˜¯"çŸ©é˜µçš„è¡Œåˆ—æ•°"
        #   çœŸæ­£çš„çŸ©é˜µå½¢çŠ¶ç”± nn.Linear(è¾“å…¥ç»´åº¦, è¾“å‡ºç»´åº¦) å†³å®š
        # 
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ å¦‚ä½•ä»è¾“å…¥ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œå½±å“æ³¨æ„åŠ›æ¨¡å¼
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆç”¨äºè®¡ç®—æ³¨æ„åŠ›æƒé‡çš„æŸ¥è¯¢è¡¨ç¤º
        # bias=Falseï¼šå‡å°‘å‚æ•°é‡ï¼Œé¿å…ä¸å¿…è¦çš„åç½®
        self.q_proj = nn.Linear(args.hidden_size, 
                               args.num_attention_heads * self.head_dim, 
                               bias=False)
        
        # ğŸ”‘ é”®æŠ•å½±çŸ©é˜µ (Key Projection Matrix)  
        # âš ï¸ é‡è¦æ¦‚å¿µè§£é‡Šï¼šself.k_proj å°±æ˜¯ä¼ ç»Ÿ Attention è®ºæ–‡ä¸­çš„ W_K æƒé‡çŸ©é˜µï¼
        # 
        # ğŸ“š ç†è®ºå¯¹åº”å…³ç³»ï¼š
        # - è®ºæ–‡ä¸­: K = X @ W_K (Xæ˜¯è¾“å…¥ï¼ŒW_Kæ˜¯é”®æƒé‡çŸ©é˜µ)
        # - ä»£ç ä¸­: xk = self.k_proj(x) (ç­‰ä»·äº x @ W_Kï¼Œbias=False)
        # 
        # ğŸ” çŸ©é˜µç»´åº¦åˆ†æï¼š
        # - W_K å½¢çŠ¶: [hidden_size, num_key_value_heads * head_dim]
        # - æ³¨æ„: åœ¨GQAä¸­ï¼Œé”®å¤´æ•°å¯èƒ½å°‘äºæŸ¥è¯¢å¤´æ•°ï¼Œå®ç°å‚æ•°å…±äº«
        # - ä¾‹å¦‚: 8ä¸ªæŸ¥è¯¢å¤´å¯èƒ½åªå¯¹åº”2ä¸ªé”®å¤´ï¼Œå‡å°‘4å€å‚æ•°é‡
        # 
        # ğŸ’¡ GQA (Grouped Query Attention) ä¼˜åŒ–ï¼š
        # - ä¼ ç»ŸMHA: num_key_heads = num_query_heads
        # - GQA: num_key_heads < num_query_heads (å‚æ•°å…±äº«)
        # - å¥½å¤„: æ˜¾è‘—å‡å°‘KVç¼“å­˜å†…å­˜ï¼ŒåŠ é€Ÿæ¨ç†
        # 
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ å¦‚ä½•ç”Ÿæˆç”¨äºåŒ¹é…çš„é”®å‘é‡
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆé”®è¡¨ç¤ºï¼Œä¸æŸ¥è¯¢è®¡ç®—ç›¸ä¼¼åº¦
        self.k_proj = nn.Linear(args.hidden_size, 
                               self.num_key_value_heads * self.head_dim, 
                               bias=False)
        
        # ğŸ’ å€¼æŠ•å½±çŸ©é˜µ (Value Projection Matrix)
        # âš ï¸ é‡è¦æ¦‚å¿µè§£é‡Šï¼šself.v_proj å°±æ˜¯ä¼ ç»Ÿ Attention è®ºæ–‡ä¸­çš„ W_V æƒé‡çŸ©é˜µï¼
        # 
        # ğŸ“š ç†è®ºå¯¹åº”å…³ç³»ï¼š
        # - è®ºæ–‡ä¸­: V = X @ W_V (Xæ˜¯è¾“å…¥ï¼ŒW_Væ˜¯å€¼æƒé‡çŸ©é˜µ)  
        # - ä»£ç ä¸­: xv = self.v_proj(x) (ç­‰ä»·äº x @ W_Vï¼Œbias=False)
        # 
        # ğŸ” çŸ©é˜µç»´åº¦åˆ†æï¼š
        # - W_V å½¢çŠ¶: [hidden_size, num_key_value_heads * head_dim] 
        # - æ³¨æ„: å€¼å¤´æ•°ä¸é”®å¤´æ•°ç›¸åŒï¼Œç¡®ä¿ä¸€ä¸€å¯¹åº”å…³ç³»
        # - æ¯ä¸ªé”®éƒ½æœ‰å¯¹åº”çš„å€¼ï¼Œä¿æŒæ³¨æ„åŠ›è®¡ç®—çš„æ•°å­¦ä¸€è‡´æ€§
        # 
        # ğŸ’¡ å€¼çŸ©é˜µçš„ç‰¹æ®Šä½œç”¨ï¼š
        # - Qå’ŒKå†³å®š"å…³æ³¨ä»€ä¹ˆ"(æ³¨æ„åŠ›æƒé‡)
        # - Vå†³å®š"ä¼ é€’ä»€ä¹ˆä¿¡æ¯"(å®é™…å†…å®¹)
        # - æœ€ç»ˆè¾“å‡º = æ³¨æ„åŠ›æƒé‡ Ã— å€¼å‘é‡ çš„åŠ æƒç»„åˆ
        # 
        # ğŸ¯ ä¸ä¼ ç»ŸMLPçš„åŒºåˆ«ï¼š
        # - æ™®é€šMLP: æ‰€æœ‰ä½ç½®ä½¿ç”¨ç›¸åŒçš„æƒé‡
        # - Attention: æ¯ä¸ªä½ç½®æ ¹æ®å…¶ä»–ä½ç½®åŠ¨æ€åŠ æƒVçŸ©é˜µçš„è¾“å‡º
        # 
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ å¦‚ä½•ç”Ÿæˆç”¨äºèšåˆçš„å€¼å‘é‡
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆæœ€ç»ˆè¾“å‡ºçš„å†…å®¹è¡¨ç¤º
        self.v_proj = nn.Linear(args.hidden_size, 
                               self.num_key_value_heads * self.head_dim, 
                               bias=False)
        
        # ğŸ¯ è¾“å‡ºæŠ•å½±çŸ©é˜µ (Output Projection Matrix)
        # çŸ©é˜µå½¢çŠ¶: [num_attention_heads * head_dim, hidden_size]
        # ä½œç”¨: å°†å¤šå¤´æ³¨æ„åŠ›çš„è¾“å‡ºé‡æ–°æŠ•å½±å›åŸå§‹éšè—ç»´åº¦
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ å¦‚ä½•æ•´åˆå¤šå¤´ä¿¡æ¯ï¼Œå½±å“æœ€ç»ˆè¾“å‡ºè´¨é‡
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆç»Ÿä¸€çš„è¾“å‡ºè¡¨ç¤º
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, 
                               args.hidden_size, 
                               bias=False)
        
        # æ³¨æ„åŠ›æƒé‡çš„dropoutï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
        # è®­ç»ƒä½œç”¨ï¼šåœ¨æ³¨æ„åŠ›æƒé‡ä¸Šåº”ç”¨éšæœºå¤±æ´»ï¼Œå¢å¼ºæ¨¡å‹é²æ£’æ€§
        # æ¨ç†ä½œç”¨ï¼šåœ¨æ¨ç†æ—¶å…³é—­ï¼Œç¡®ä¿è¾“å‡ºçš„ç¡®å®šæ€§
        self.attn_dropout = nn.Dropout(args.dropout)
        
        # æ®‹å·®è¿æ¥çš„dropoutï¼šå¯¹è¾“å‡ºè¿›è¡Œæ­£åˆ™åŒ–
        # è®­ç»ƒä½œç”¨ï¼šåœ¨æ®‹å·®è·¯å¾„ä¸Šåº”ç”¨dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        # æ¨ç†ä½œç”¨ï¼šæ¨ç†æ—¶è‡ªåŠ¨å…³é—­
        self.resid_dropout = nn.Dropout(args.dropout)
        
        # ç¼“å­˜dropoutç‡ä»¥ä¾›åç»­ä½¿ç”¨
        self.dropout = args.dropout
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒFlash Attentionå¹¶æ ¹æ®é…ç½®å¯ç”¨
        # è®­ç»ƒä½œç”¨ï¼šå¯ç”¨å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—ï¼Œæ”¯æŒæ›´é•¿åºåˆ—çš„è®­ç»ƒ
        # æ¨ç†ä½œç”¨ï¼šæ˜¾è‘—é™ä½æ¨ç†æ—¶çš„å†…å­˜å ç”¨å’Œè®¡ç®—æ—¶é—´
        # hasattræ£€æŸ¥ï¼šç¡®ä¿PyTorchç‰ˆæœ¬æ”¯æŒï¼ˆéœ€è¦PyTorch >= 2.0ï¼‰
        self.flash = (hasattr(torch.nn.functional, 'scaled_dot_product_attention') 
                     and args.flash_attn)

    def forward(self,
                x: torch.Tensor,
                position_embeddings: Tuple[torch.Tensor, torch.Tensor],
                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                use_cache=False,
                attention_mask: Optional[torch.Tensor] = None):
        """
        æ³¨æ„åŠ›æœºåˆ¶çš„å‰å‘ä¼ æ’­
        
        Args:
            x (torch.Tensor): è¾“å…¥éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, hidden_size)
            position_embeddings (Tuple): RoPEçš„ä½™å¼¦å’Œæ­£å¼¦ç¼–ç  (cos, sin)
            past_key_value (Optional[Tuple]): KVç¼“å­˜ï¼Œç”¨äºç”Ÿæˆæ—¶çš„å¢é‡è§£ç 
            use_cache (bool): æ˜¯å¦è¿”å›KVç¼“å­˜ä¾›ä¸‹æ¬¡ä½¿ç”¨
            attention_mask (Optional[torch.Tensor]): æ³¨æ„åŠ›æ©ç 
            
        Returns:
            Tuple[torch.Tensor, Optional[Tuple]]: (è¾“å‡ºéšè—çŠ¶æ€, KVç¼“å­˜)
        """
        # è·å–è¾“å…¥å¼ é‡çš„åŸºæœ¬ç»´åº¦
        # è®­ç»ƒä½œç”¨ï¼šåŠ¨æ€é€‚åº”ä¸åŒæ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦ï¼Œæ”¯æŒçµæ´»çš„è®­ç»ƒé…ç½®
        # æ¨ç†ä½œç”¨ï¼šå¤„ç†å¯å˜é•¿åº¦è¾“å…¥ï¼Œé€‚åº”å®é™…åº”ç”¨åœºæ™¯
        bsz, seq_len, _ = x.shape
        
        # === QKVçŸ©é˜µå˜æ¢è®¡ç®—è¿‡ç¨‹ ===
        # è¿™é‡Œå±•ç¤ºäº†ä»è¾“å…¥åˆ°æ³¨æ„åŠ›è®¡ç®—çš„å®Œæ•´QKVå˜æ¢æµç¨‹
        
        # ğŸ”„ æ­¥éª¤1: çº¿æ€§æŠ•å½±å˜æ¢ - é€šè¿‡æƒé‡çŸ©é˜µç”ŸæˆQã€Kã€V
        # è¾“å…¥: x [batch_size, seq_len, hidden_size]
        # è¾“å‡º: xq [batch_size, seq_len, n_heads * head_dim]
        #      xk [batch_size, seq_len, n_kv_heads * head_dim] 
        #      xv [batch_size, seq_len, n_kv_heads * head_dim]
        # æ•°å­¦å…¬å¼: Q = x @ W_q, K = x @ W_k, V = x @ W_v
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ å¦‚ä½•å°†è¾“å…¥è½¬æ¢ä¸ºæ³¨æ„åŠ›æœºåˆ¶æ‰€éœ€çš„ä¸‰ç§è¡¨ç¤º
        # æ¨ç†ä½œç”¨ï¼šæ ¹æ®å­¦ä¹ åˆ°çš„æƒé‡ç”ŸæˆæŸ¥è¯¢ã€é”®ã€å€¼å‘é‡
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        
        # ğŸ”„ æ­¥éª¤2: é‡å¡‘ä¸ºå¤šå¤´æ³¨æ„åŠ›æ ¼å¼
        # å°†ä¸€ç»´çš„æŠ•å½±ç»“æœé‡å¡‘ä¸ºå¤šå¤´ç»“æ„ï¼Œæ¯ä¸ªå¤´å¤„ç†éƒ¨åˆ†ç‰¹å¾
        # è¾“å‡ºå½¢çŠ¶: [batch_size, seq_len, num_heads, head_dim]
        # è®­ç»ƒä½œç”¨ï¼šç»„ç»‡å¼ é‡ç»“æ„ä»¥æ”¯æŒå¤šå¤´å¹¶è¡Œè®­ç»ƒï¼Œæé«˜è®­ç»ƒæ•ˆç‡
        # æ¨ç†ä½œç”¨ï¼šä¸ºé«˜æ•ˆçš„å¤šå¤´æ³¨æ„åŠ›è®¡ç®—å‡†å¤‡æ•°æ®æ ¼å¼
        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)       # æŸ¥è¯¢ï¼šå®Œæ•´å¤´æ•°
        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)    # é”®ï¼šGQAå‡å°‘çš„å¤´æ•°
        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)    # å€¼ï¼šä¸é”®ç›¸åŒå¤´æ•°

        # === RoPEä½ç½®ç¼–ç çš„è§£åŒ…æ“ä½œ ===
        # ğŸ¯ å…³é”®ç†è§£ï¼šposition_embeddings çš„æ¥æºå’Œè°ƒç”¨æœºåˆ¶è¯¦è§£
        # ==========================================================
        # 
        # ğŸ“‹ å‚æ•°ä¼ é€’é“¾åˆ†æï¼š
        # -----------------
        # æ‚¨çš„ç–‘é—®å¾ˆæœ‰æ„æ€ï¼è®©æˆ‘ä»¬è¿½è¸ª position_embeddings çš„å®Œæ•´è°ƒç”¨é“¾ï¼š
        # 
        # 1ï¸âƒ£ **MiniMindModel.forward()** (ä¸»æ¨¡å‹) 
        #    â†“ åœ¨è¿™é‡Œè®¡ç®—å’Œå‡†å¤‡ä½ç½®ç¼–ç 
        #    position_embeddings = (
        #        self.freqs_cos[start_pos:start_pos + seq_length],
        #        self.freqs_sin[start_pos:start_pos + seq_length]
        #    )
        # 
        # 2ï¸âƒ£ **MiniMindBlock.forward()** (Transformerå±‚)
        #    â†“ æ¥æ”¶å¹¶ä¼ é€’ä½ç½®ç¼–ç 
        #    hidden_states, present = layer(hidden_states, position_embeddings, ...)
        # 
        # 3ï¸âƒ£ **Attention.forward()** (æ³¨æ„åŠ›å±‚) â† æˆ‘ä»¬ç°åœ¨æ‰€åœ¨çš„ä½ç½®
        #    â†“ è§£åŒ…å¹¶ä½¿ç”¨ä½ç½®ç¼–ç 
        #    cos, sin = position_embeddings  # è¿™å°±æ˜¯å½“å‰è¿™è¡Œä»£ç ï¼
        #
        # ğŸ” ä¸ºä»€ä¹ˆç”¨å…ƒç»„ (cos, sin) ä¼ é€’ï¼Ÿ
        # ------------------------------
        # RoPEä½ç½®ç¼–ç éœ€è¦ä½™å¼¦å’Œæ­£å¼¦ä¸¤ä¸ªåˆ†é‡ï¼š
        # - cos: ç”¨äºå®éƒ¨çš„æ—‹è½¬å˜æ¢
        # - sin: ç”¨äºè™šéƒ¨çš„æ—‹è½¬å˜æ¢
        # - æ•°å­¦åŸç†ï¼šå¤æ•°æ—‹è½¬ e^(iÎ¸) = cos(Î¸) + iÂ·sin(Î¸)
        # 
        # ğŸ“ å¼ é‡å½¢çŠ¶åˆ†æï¼š
        # ---------------
        # cos å’Œ sin çš„å½¢çŠ¶éƒ½æ˜¯ï¼š[seq_length, head_dim//2]
        # - seq_length: å½“å‰åºåˆ—é•¿åº¦
        # - head_dim//2: å› ä¸ºRoPEåªåº”ç”¨äºæ¯ä¸ªå¤´ç»´åº¦çš„ä¸€åŠ
        # 
        # ğŸ’¡ ä¸nn.Moduleç»§æ‰¿çš„å…³ç³»ï¼š
        # -------------------------
        # æ‚¨é—®å¾—å¾ˆå¥½ï¼è¿™é‡Œä¸nn.Moduleç»§æ‰¿å…³ç³»ä¸å¤§ï¼Œä¸»è¦æ˜¯ï¼š
        # 1. **å‡½æ•°å‚æ•°ä¼ é€’**ï¼šposition_embeddingsæ˜¯forward()æ–¹æ³•çš„å‚æ•°
        # 2. **ä¸Šä¸‹çº§è°ƒç”¨**ï¼šä¸Šå±‚MiniMindModelè®¡ç®—å¥½åä¼ é€’ä¸‹æ¥
        # 3. **è®¾è®¡æ¨¡å¼**ï¼šéµå¾ª"è°è®¡ç®—è°ä¼ é€’"çš„åŸåˆ™
        #    - MiniMindModelè´Ÿè´£è®¡ç®—æ‰€æœ‰å±‚å…±äº«çš„ä½ç½®ç¼–ç 
        #    - å„ä¸ªAttentionå±‚è´Ÿè´£ä½¿ç”¨ä½ç½®ç¼–ç 
        # 
        # ğŸš€ ä¸ºä»€ä¹ˆä¸åœ¨Attentionå†…éƒ¨è®¡ç®—ä½ç½®ç¼–ç ï¼Ÿ
        # ------------------------------------------
        # 1. **æ€§èƒ½ä¼˜åŒ–**ï¼šé¿å…æ¯ä¸ªæ³¨æ„åŠ›å±‚é‡å¤è®¡ç®—ç›¸åŒçš„ä½ç½®ç¼–ç 
        # 2. **å†…å­˜æ•ˆç‡**ï¼šåœ¨æ¨¡å‹å±‚çº§è®¡ç®—ä¸€æ¬¡ï¼Œæ‰€æœ‰å±‚å…±äº«ä½¿ç”¨
        # 3. **ç¼“å­˜å‹å¥½**ï¼šä½ç½®ç¼–ç å¯ä»¥é¢„è®¡ç®—å¹¶ç¼“å­˜
        # 4. **å¢é‡ç”Ÿæˆæ”¯æŒ**ï¼šä¾¿äºå¤„ç†KVç¼“å­˜ä¸­çš„ä½ç½®å¯¹é½
        # 
        # ğŸ”„ å®é™…æ‰§è¡Œæµç¨‹ï¼š
        # ---------------
        # è®­ç»ƒæ—¶ï¼š
        # MiniMindModel.forward() â†’ è®¡ç®—å®Œæ•´åºåˆ—çš„cos,sin
        # â†’ ä¼ é€’ç»™æ¯ä¸ªMiniMindBlock â†’ ä¼ é€’ç»™Attention â†’ è§£åŒ…ä½¿ç”¨
        # 
        # æ¨ç†æ—¶ï¼ˆå¢é‡ç”Ÿæˆï¼‰ï¼š
        # MiniMindModel.forward() â†’ åªè®¡ç®—æ–°tokençš„cos,sin  
        # â†’ è€ƒè™‘start_posåç§» â†’ ä¼ é€’ç»™Attention â†’ è§£åŒ…ä½¿ç”¨
        #
        # è®­ç»ƒä½œç”¨ï¼šä¸ºQå’ŒKå¼ é‡æä¾›ä½ç½®æ„ŸçŸ¥èƒ½åŠ›ï¼Œè®©æ¨¡å‹ç†è§£tokençš„ç›¸å¯¹ä½ç½®
        # æ¨ç†ä½œç”¨ï¼šç¡®ä¿ç”Ÿæˆæ—¶ä½ç½®ç¼–ç çš„è¿ç»­æ€§å’Œæ­£ç¡®æ€§
        cos, sin = position_embeddings
        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])

        # === KVç¼“å­˜æœºåˆ¶ ===
        # åœ¨ç”Ÿæˆé˜¶æ®µï¼Œå°†å†å²çš„é”®å€¼ä¸å½“å‰é”®å€¼æ‹¼æ¥ï¼Œé¿å…é‡å¤è®¡ç®—
        # è®­ç»ƒä½œç”¨ï¼šè®­ç»ƒæ—¶é€šå¸¸ä¸ä½¿ç”¨ï¼Œå› ä¸ºæœ‰å®Œæ•´åºåˆ—
        # æ¨ç†ä½œç”¨ï¼šæ˜¾è‘—åŠ é€Ÿå¢é‡ç”Ÿæˆï¼Œé¿å…é‡å¤è®¡ç®—å†å²tokençš„KV
        if past_key_value is not None:
            # åœ¨åºåˆ—ç»´åº¦ä¸Šæ‹¼æ¥å†å²KVç¼“å­˜
            xk = torch.cat([past_key_value[0], xk], dim=1)  # æ‹¼æ¥å†å²é”®
            xv = torch.cat([past_key_value[1], xv], dim=1)  # æ‹¼æ¥å†å²å€¼
        
        # æ ¹æ®éœ€è¦ä¿å­˜å½“å‰çš„KVçŠ¶æ€ä¾›ä¸‹æ¬¡ä½¿ç”¨
        # è®­ç»ƒä½œç”¨ï¼šè®­ç»ƒæ—¶é€šå¸¸è®¾ä¸ºNone
        # æ¨ç†ä½œç”¨ï¼šä¿å­˜çŠ¶æ€ä»¥æ”¯æŒé«˜æ•ˆçš„å¢é‡ç”Ÿæˆ
        past_kv = (xk, xv) if use_cache else None

        # === åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰å¤„ç† ===
        # è½¬ç½®ç»´åº¦å¹¶é‡å¤KVä»¥åŒ¹é…æŸ¥è¯¢å¤´æ•°é‡
        # è®­ç»ƒä½œç”¨ï¼šå®ç°å‚æ•°é«˜æ•ˆçš„å¤šå¤´æ³¨æ„åŠ›ï¼Œå‡å°‘å†…å­˜éœ€æ±‚
        # æ¨ç†ä½œç”¨ï¼šæ˜¾è‘—é™ä½KVç¼“å­˜çš„å†…å­˜å ç”¨
        xq, xk, xv = (
            xq.transpose(1, 2),                               # Q: [batch, n_heads, seq, head_dim]
            repeat_kv(xk, self.n_rep).transpose(1, 2),        # K: é‡å¤åè½¬ç½®
            repeat_kv(xv, self.n_rep).transpose(1, 2)         # V: é‡å¤åè½¬ç½®
        )

        # === æ³¨æ„åŠ›è®¡ç®—ï¼šé€‰æ‹©é«˜æ•ˆå®ç° ===
        if self.flash and seq_len != 1:
            # ä½¿ç”¨Flash Attentionï¼šå†…å­˜é«˜æ•ˆçš„èåˆå®ç°
            # è®­ç»ƒä½œç”¨ï¼šæ”¯æŒæ›´é•¿åºåˆ—è®­ç»ƒï¼Œå‡å°‘å†…å­˜ç“¶é¢ˆï¼ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹
            # æ¨ç†ä½œç”¨ï¼šæ˜¾è‘—é™ä½æ¨ç†æ—¶çš„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ—¶é—´
            dropout_p = self.dropout if self.training else 0.0  # æ¨ç†æ—¶å…³é—­dropout
            attn_mask = None
            
            # å¤„ç†æ³¨æ„åŠ›æ©ç ï¼šæ‰©å±•åˆ°å¤šå¤´æ ¼å¼
            # è®­ç»ƒä½œç”¨ï¼šæ­£ç¡®åº”ç”¨å¡«å……æ©ç ï¼Œé¿å…å…³æ³¨å¡«å……token
            # æ¨ç†ä½œç”¨ï¼šç¡®ä¿ç”Ÿæˆè´¨é‡ï¼Œé˜²æ­¢å…³æ³¨æ— æ•ˆä½ç½®
            if attention_mask is not None:
                # æ‰©å±•æ©ç ç»´åº¦ä»¥åŒ¹é…å¤šå¤´æ³¨æ„åŠ›çš„å½¢çŠ¶
                attn_mask = attention_mask.view(bsz, 1, 1, -1).expand(bsz, self.n_local_heads, seq_len, -1)
                attn_mask = attn_mask.bool() if attention_mask is not None else None

            # æ‰§è¡ŒFlash Attentionè®¡ç®—
            # is_causal=Trueï¼šè‡ªåŠ¨åº”ç”¨å› æœæ©ç ï¼Œç¡®ä¿è‡ªå›å½’ç‰¹æ€§
            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=True)
        else:
            # === ä¼ ç»Ÿæ³¨æ„åŠ›è®¡ç®— ===
            # ç”¨äºåºåˆ—é•¿åº¦ä¸º1çš„æƒ…å†µï¼ˆå¢é‡ç”Ÿæˆï¼‰æˆ–ä¸æ”¯æŒFlash Attentionæ—¶
            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šQ @ K^T / sqrt(d_k)
            # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ tokené—´çš„å…³è”å¼ºåº¦ï¼Œæ„å»ºæ³¨æ„åŠ›æ¨¡å¼
            # æ¨ç†ä½œç”¨ï¼šç¡®å®šå½“å‰tokenåº”è¯¥å…³æ³¨å“ªäº›å†å²ä¿¡æ¯
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
            
            # æ·»åŠ å› æœæ©ç ï¼šé˜²æ­¢æ¨¡å‹çœ‹åˆ°æœªæ¥ä¿¡æ¯
            # è®­ç»ƒä½œç”¨ï¼šç¡®ä¿æ¨¡å‹åªèƒ½ä½¿ç”¨å†å²ä¿¡æ¯è¿›è¡Œé¢„æµ‹ï¼Œç»´æŒå› æœå…³ç³»
            # æ¨ç†ä½œç”¨ï¼šä¿æŒè¯­è¨€æ¨¡å‹çš„è‡ªå›å½’ç”Ÿæˆç‰¹æ€§
            scores = scores + torch.triu(
                torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
                diagonal=1  # å¯¹è§’çº¿ä¸Šæ–¹è®¾ä¸ºè´Ÿæ— ç©·
            ).unsqueeze(0).unsqueeze(0)  # æ‰©å±•åˆ°æ‰¹æ¬¡å’Œå¤´ç»´åº¦

            # åº”ç”¨é¢å¤–çš„æ³¨æ„åŠ›æ©ç ï¼ˆå¦‚å¡«å……æ©ç ï¼‰
            # è®­ç»ƒä½œç”¨ï¼šé¿å…æ¨¡å‹å…³æ³¨å¡«å……ä½ç½®ï¼Œæé«˜è®­ç»ƒæ•ˆæœ
            # æ¨ç†ä½œç”¨ï¼šç¡®ä¿ç”Ÿæˆæ—¶ä¸ä¼šå…³æ³¨æ— æ•ˆçš„å¡«å……token
            if attention_mask is not None:
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9  # å°†0ä½ç½®è®¾ä¸ºæå°å€¼
                scores = scores + extended_attention_mask

            # è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼šsoftmaxå½’ä¸€åŒ–
            # è®­ç»ƒä½œç”¨ï¼šå°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œæ”¯æŒæ¢¯åº¦ä¼ æ’­
            # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆç”¨äºä¿¡æ¯èšåˆçš„æƒé‡åˆ†å¸ƒ
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            
            # åº”ç”¨æ³¨æ„åŠ›dropoutï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
            # è®­ç»ƒä½œç”¨ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›
            # æ¨ç†ä½œç”¨ï¼šæ¨ç†æ—¶è‡ªåŠ¨è·³è¿‡
            scores = self.attn_dropout(scores)
            
            # è®¡ç®—åŠ æƒçš„å€¼å‘é‡ï¼šAttention @ V
            # è®­ç»ƒä½œç”¨ï¼šæ ¹æ®æ³¨æ„åŠ›æƒé‡èšåˆä¿¡æ¯ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡åŒ–è¡¨ç¤º
            # æ¨ç†ä½œç”¨ï¼šäº§ç”Ÿèåˆäº†ç›¸å…³å†å²ä¿¡æ¯çš„è¾“å‡º
            output = scores @ xv

        # === è¾“å‡ºå¤„ç† ===
        # å°†å¤šå¤´è¾“å‡ºåˆå¹¶å›åŸå§‹æ ¼å¼
        # è®­ç»ƒä½œç”¨ï¼šå°†å¤šå¤´ä¿¡æ¯èåˆä¸ºç»Ÿä¸€çš„éšè—è¡¨ç¤º
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆæ ‡å‡†æ ¼å¼çš„è¾“å‡ºä¾›åç»­å±‚ä½¿ç”¨
        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)
        
        # é€šè¿‡è¾“å‡ºæŠ•å½±å±‚å¹¶åº”ç”¨æ®‹å·®dropout
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ å¦‚ä½•æ•´åˆå¤šå¤´æ³¨æ„åŠ›ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œæ­£åˆ™åŒ–
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆæœ€ç»ˆçš„æ³¨æ„åŠ›è¾“å‡º
        output = self.resid_dropout(self.o_proj(output))
        
        return output, past_kv


class FeedForward(nn.Module):
    """
    å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰
    ===================================
    
    é¡¹ç›®ä¸­çš„ä½œç”¨ï¼š
    - ä½œä¸ºTransformerå—çš„éçº¿æ€§å˜æ¢ç»„ä»¶ï¼Œæä¾›æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›
    - å®ç°SwiGLUæ¿€æ´»æœºåˆ¶ï¼Œå¢å¼ºæ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›
    - ä¸ºæ¯ä¸ªä½ç½®çš„tokenæä¾›ç‹¬ç«‹çš„ç‰¹å¾å˜æ¢
    
    å¤§æ¨¡å‹æ¡†æ¶ä¸­çš„ä½œç”¨ï¼š
    - æ‰¿æ‹…Transformeræ¶æ„ä¸­çš„æ ¸å¿ƒè®¡ç®—ä»»åŠ¡ï¼Œå ç”¨æ¨¡å‹å¤§éƒ¨åˆ†å‚æ•°
    - å®ç°ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ ‡å‡†FFNè®¾è®¡ï¼ˆSwiGLUæ¿€æ´»ï¼‰
    - æä¾›æ¨¡å‹çš„ä¸»è¦éçº¿æ€§è¡¨è¾¾èƒ½åŠ›å’Œè®°å¿†å­˜å‚¨åŠŸèƒ½
    - æ”¯æŒé«˜æ•ˆçš„å‚æ•°ç¼©æ”¾å’Œæ¨¡å‹å®¹é‡æ‰©å±•
    
    æ¶æ„ç‰¹ç‚¹ï¼š
    1. SwiGLUæ¿€æ´»å‡½æ•°ï¼š
       - ç»“åˆSwishæ¿€æ´»å’Œé—¨æ§çº¿æ€§å•å…ƒï¼ˆGLUï¼‰
       - ç›¸æ¯”ä¼ ç»ŸReLUå…·æœ‰æ›´å¥½çš„æ€§èƒ½å’Œæ¢¯åº¦ç‰¹æ€§
       - è¢«GPT-3.5ã€LLaMAç­‰ä¸»æµæ¨¡å‹å¹¿æ³›é‡‡ç”¨
    
    2. ä¸‰çº¿æ€§å±‚è®¾è®¡ï¼š
       - gate_projï¼šé—¨æ§æŠ•å½±ï¼Œæ§åˆ¶ä¿¡æ¯æµ
       - up_projï¼šä¸Šé‡‡æ ·æŠ•å½±ï¼Œæ‰©å±•ç‰¹å¾ç»´åº¦
       - down_projï¼šä¸‹é‡‡æ ·æŠ•å½±ï¼Œæ¢å¤åŸå§‹ç»´åº¦
    
    3. ç»´åº¦ç¼©æ”¾ç­–ç•¥ï¼š
       - ä¸­é—´å±‚ç»´åº¦é€šå¸¸ä¸ºéšè—å±‚çš„8/3å€
       - å¯¹é½åˆ°64çš„å€æ•°ï¼Œä¼˜åŒ–è®¡ç®—æ•ˆç‡
    
    è®¡ç®—å…¬å¼ï¼š
    FFN(x) = down_proj(SiLU(gate_proj(x)) âŠ™ up_proj(x))
    
    æ€§èƒ½ç‰¹æ€§ï¼š
    - å‚æ•°é‡å æ•´ä¸ªæ¨¡å‹çš„2/3å·¦å³
    - è®¡ç®—å¯†é›†å‹ï¼Œé€‚åˆGPUå¹¶è¡Œä¼˜åŒ–
    - æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦è®­ç»ƒ
    
    é€‚ç”¨åœºæ™¯ï¼š
    - è¯­è¨€æ¨¡å‹çš„æ ‡å‡†å‰é¦ˆå±‚
    - éœ€è¦å¼ºéçº¿æ€§è¡¨è¾¾çš„åºåˆ—å»ºæ¨¡ä»»åŠ¡
    - å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œå¾®è°ƒåœºæ™¯
    """
    
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ–å‰é¦ˆç½‘ç»œ
        
        Args:
            config (MiniMindConfig): æ¨¡å‹é…ç½®å‚æ•°
        """
        super().__init__()
        
        # è®¡ç®—ä¸­é—´å±‚ç»´åº¦ï¼šå¦‚æœæœªæŒ‡å®šåˆ™æŒ‰8/3å€éšè—ç»´åº¦è®¾ç½®
        # è®­ç»ƒä½œç”¨ï¼šç¡®å®šå‰é¦ˆç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›å’Œå‚æ•°é‡
        # æ¨ç†ä½œç”¨ï¼šå½±å“æ¨ç†æ—¶çš„è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚
        if config.intermediate_size is None:
            # 8/3å€çš„ç»éªŒæ¯”ä¾‹ï¼šæ¥è‡ªTransformeræ¶æ„çš„æœ€ä½³å®è·µ
            intermediate_size = int(config.hidden_size * 8 / 3)
            # å¯¹é½åˆ°64çš„å€æ•°ï¼šä¼˜åŒ–GPU tensor coreè®¡ç®—æ•ˆç‡
            # è®­ç»ƒä½œç”¨ï¼šåŠ é€ŸçŸ©é˜µä¹˜æ³•è¿ç®—ï¼Œæé«˜è®­ç»ƒååé‡
            # æ¨ç†ä½œç”¨ï¼šå……åˆ†åˆ©ç”¨ç¡¬ä»¶ç‰¹æ€§ï¼Œæå‡æ¨ç†é€Ÿåº¦
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
            
        # === SwiGLUæ¶æ„çš„ä¸‰ä¸ªçº¿æ€§å±‚ ===
        # é—¨æ§æŠ•å½±å±‚ï¼šç”Ÿæˆé—¨æ§ä¿¡å·ï¼Œæ§åˆ¶ä¿¡æ¯æµ
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ å“ªäº›ç‰¹å¾åº”è¯¥è¢«æ¿€æ´»æˆ–æŠ‘åˆ¶
        # æ¨ç†ä½œç”¨ï¼šåŠ¨æ€è°ƒèŠ‚ä¿¡æ¯ä¼ é€’çš„å¼ºåº¦
        # bias=Falseï¼šå‡å°‘å‚æ•°é‡ï¼Œé¿å…å¼•å…¥é¢å¤–åç½®
        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        
        # ä¸‹é‡‡æ ·æŠ•å½±å±‚ï¼šå°†æ‰©å±•çš„ç‰¹å¾æ˜ å°„å›åŸå§‹ç»´åº¦
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ å¦‚ä½•æ•´åˆä¸­é—´å±‚çš„ä¸°å¯Œç‰¹å¾
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆæœ€ç»ˆçš„å‰é¦ˆç½‘ç»œè¾“å‡º
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)
        
        # ä¸Šé‡‡æ ·æŠ•å½±å±‚ï¼šå°†è¾“å…¥æ‰©å±•åˆ°ä¸­é—´ç»´åº¦ç©ºé—´
        # è®­ç»ƒä½œç”¨ï¼šå¢åŠ ç‰¹å¾è¡¨è¾¾çš„ç»´åº¦ï¼Œæä¾›æ›´å¼ºçš„éçº¿æ€§å˜æ¢èƒ½åŠ›
        # æ¨ç†ä½œç”¨ï¼šä¸ºé—¨æ§æœºåˆ¶æä¾›ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤º
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        
        # Dropoutå±‚ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
        # è®­ç»ƒä½œç”¨ï¼šåœ¨å‰é¦ˆç½‘ç»œè¾“å‡ºä¸Šåº”ç”¨éšæœºå¤±æ´»ï¼Œå¢å¼ºæ³›åŒ–èƒ½åŠ›
        # æ¨ç†ä½œç”¨ï¼šæ¨ç†æ—¶è‡ªåŠ¨å…³é—­ï¼Œç¡®ä¿è¾“å‡ºçš„ç¡®å®šæ€§
        self.dropout = nn.Dropout(config.dropout)
        
        # æ¿€æ´»å‡½æ•°é€‰æ‹©ï¼šä»transformersåº“çš„æ˜ å°„ä¸­è·å–
        # ACT2FNè¯¦ç»†è¯´æ˜ï¼š
        # ACT2FNæ˜¯transformersåº“æä¾›çš„æ¿€æ´»å‡½æ•°æ˜ å°„å­—å…¸
        # å°†é…ç½®æ–‡ä»¶ä¸­çš„å­—ç¬¦ä¸²æ˜ å°„åˆ°å¯¹åº”çš„PyTorchæ¿€æ´»å‡½æ•°
        # æ”¯æŒçš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ï¼š
        # - "relu": torch.nn.functional.relu
        # - "gelu": torch.nn.functional.gelu  
        # - "silu": torch.nn.functional.silu (ä¹Ÿç§°ä¸ºSwish)
        # - "swish": torch.nn.functional.silu (siluçš„åˆ«å)
        # - "tanh": torch.tanh
        # - "sigmoid": torch.sigmoid
        # ä½¿ç”¨æ–¹å¼ï¼šACT2FN[config.hidden_act] è·å–å¯¹åº”çš„æ¿€æ´»å‡½æ•°
        # åœ¨MiniMindä¸­é€šå¸¸ä½¿ç”¨'silu'æ¿€æ´»å‡½æ•°ï¼Œå³Swishæ¿€æ´»
        # è®­ç»ƒä½œç”¨ï¼šæä¾›å¹³æ»‘çš„æ¢¯åº¦ç‰¹æ€§ï¼Œæœ‰åŠ©äºä¼˜åŒ–æ”¶æ•›
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆéçº¿æ€§å˜æ¢ï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›
        self.act_fn = ACT2FN[config.hidden_act]  # æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸ä¸ºSiLU

    def forward(self, x):
        """
        å‰é¦ˆç½‘ç»œçš„å‰å‘ä¼ æ’­
        
        å®ç°SwiGLUæ¿€æ´»ï¼šgate_proj(x) * act_fn * up_proj(x)
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, hidden_size)
            
        Returns:
            torch.Tensor: è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        """
        # === SwiGLUæ¿€æ´»æœºåˆ¶å®ç° ===
        # å…¬å¼ï¼šdown_proj(act_fn(gate_proj(x)) âŠ™ up_proj(x))
        
        # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—é—¨æ§ä¿¡å· gate_proj(x)
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ æ§åˆ¶ä¿¡æ¯æµçš„é—¨æ§æƒé‡
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆç”¨äºè°ƒèŠ‚ç‰¹å¾æ¿€æ´»å¼ºåº¦çš„é—¨æ§å‘é‡
        gate_output = self.gate_proj(x)
        
        # ç¬¬äºŒæ­¥ï¼šè®¡ç®—ä¸Šé‡‡æ ·ç‰¹å¾ up_proj(x)
        # è®­ç»ƒä½œç”¨ï¼šå°†è¾“å…¥æŠ•å½±åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ï¼Œå¢å¼ºè¡¨è¾¾èƒ½åŠ›
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆä¸°å¯Œçš„ä¸­é—´ç‰¹å¾è¡¨ç¤º
        up_output = self.up_proj(x)
        
        # ç¬¬ä¸‰æ­¥ï¼šåº”ç”¨æ¿€æ´»å‡½æ•°åˆ°é—¨æ§ä¿¡å·
        # è®­ç»ƒä½œç”¨ï¼šå¼•å…¥éçº¿æ€§å˜æ¢ï¼Œæä¾›å¹³æ»‘çš„æ¢¯åº¦ç‰¹æ€§
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆå¹³æ»‘çš„é—¨æ§æƒé‡ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
        activated_gate = self.act_fn(gate_output)
        
        # ç¬¬å››æ­¥ï¼šé—¨æ§èåˆ - å…ƒç´ çº§åˆ«çš„ä¹˜æ³•
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ ç‰¹å¾é€‰æ‹©å’ŒåŠ æƒæœºåˆ¶ï¼Œæé«˜æ¨¡å‹è¡¨è¾¾èƒ½åŠ›
        # æ¨ç†ä½œç”¨ï¼šåŠ¨æ€è°ƒèŠ‚å“ªäº›ç‰¹å¾åº”è¯¥è¢«æ¿€æ´»æˆ–æŠ‘åˆ¶
        gated_features = activated_gate * up_output
        
        # ç¬¬äº”æ­¥ï¼šä¸‹é‡‡æ ·æŠ•å½±ï¼Œæ¢å¤åŸå§‹ç»´åº¦
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ å¦‚ä½•å°†é«˜ç»´ç‰¹å¾æ•´åˆå›éšè—ç»´åº¦
        # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆæœ€ç»ˆçš„å‰é¦ˆç½‘ç»œè¾“å‡º
        output = self.down_proj(gated_features)
        
        # ç¬¬å…­æ­¥ï¼šåº”ç”¨dropoutè¿›è¡Œæ­£åˆ™åŒ–
        # è®­ç»ƒä½œç”¨ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›
        # æ¨ç†ä½œç”¨ï¼šæ¨ç†æ—¶è‡ªåŠ¨è·³è¿‡ï¼Œç¡®ä¿è¾“å‡ºç¨³å®šæ€§
        return self.dropout(output)


class MoEGate(nn.Module):
    """
    ä¸“å®¶æ··åˆé—¨æ§ç½‘ç»œï¼ˆMixture of Experts Gating Networkï¼‰
    ====================================================
    
    é¡¹ç›®ä¸­çš„ä½œç”¨ï¼š
    - ä½œä¸ºMoEæ¶æ„çš„æ™ºèƒ½è·¯ç”±å™¨ï¼Œä¸ºæ¯ä¸ªtokené€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶ç½‘ç»œ
    - å®ç°ç¨€ç–æ¿€æ´»æœºåˆ¶ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å¤§å¹…æå‡æ¨¡å‹å®¹é‡
    - æä¾›ä¸“å®¶è´Ÿè½½å¹³è¡¡åŠŸèƒ½ï¼Œç¡®ä¿å„ä¸“å®¶å¾—åˆ°å‡åŒ€è®­ç»ƒ
    
    å¤§æ¨¡å‹æ¡†æ¶ä¸­çš„ä½œç”¨ï¼š
    - å®ç°ç°ä»£å¤§æ¨¡å‹çš„ç¨€ç–æ¿€æ´»èŒƒå¼ï¼Œçªç ´ä¼ ç»Ÿå¯†é›†æ¨¡å‹çš„è®¡ç®—ç“¶é¢ˆ
    - æ”¯æŒæ¨¡å‹å®¹é‡ä¸è®¡ç®—æˆæœ¬çš„è§£è€¦ï¼Œå®ç°æ›´é«˜æ•ˆçš„å‚æ•°åˆ©ç”¨
    - æä¾›åŠ¨æ€è®¡ç®—è·¯å¾„é€‰æ‹©ï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œä¸“ä¸šåŒ–ç¨‹åº¦
    - ä¸ºè¶…å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæä¾›å¯è¡Œçš„æŠ€æœ¯è·¯å¾„
    
    æ ¸å¿ƒæœºåˆ¶ï¼š
    1. æ™ºèƒ½è·¯ç”±ç®—æ³•ï¼š
       - åŸºäºtokenéšè—çŠ¶æ€è®¡ç®—ä¸“å®¶é€‰æ‹©æ¦‚ç‡
       - æ”¯æŒtop-kä¸“å®¶é€‰æ‹©ç­–ç•¥
       - å¯é…ç½®çš„è¯„åˆ†å‡½æ•°ï¼ˆsoftmaxç­‰ï¼‰
    
    2. è´Ÿè½½å¹³è¡¡æœºåˆ¶ï¼š
       - è¾…åŠ©æŸå¤±å‡½æ•°ç¡®ä¿ä¸“å®¶è´Ÿè½½å‡è¡¡
       - æ”¯æŒåºåˆ—çº§å’Œtokençº§å¹³è¡¡ç­–ç•¥
       - é˜²æ­¢ä¸“å®¶å¡Œé™·å’Œè¿‡åº¦ä¸“ä¸šåŒ–
    
    3. ç¨€ç–æ¿€æ´»ä¼˜åŒ–ï¼š
       - æ¯ä¸ªtokenä»…æ¿€æ´»å°‘æ•°ä¸“å®¶ï¼ˆé€šå¸¸2-4ä¸ªï¼‰
       - æ˜¾è‘—å‡å°‘å®é™…è®¡ç®—é‡
       - ä¿æŒæ¨¡å‹æ€»å®¹é‡çš„å¤§å¹…æå‡
    
    æŠ€æœ¯ä¼˜åŠ¿ï¼š
    - è®¡ç®—æ•ˆç‡ï¼šä»…æ¿€æ´»éƒ¨åˆ†ä¸“å®¶ï¼Œé™ä½æ¨ç†æˆæœ¬
    - æ¨¡å‹å®¹é‡ï¼šæ”¯æŒæ•°ç™¾ä¸ªä¸“å®¶ï¼Œå¤§å¹…å¢åŠ å‚æ•°é‡
    - ä¸“ä¸šåŒ–èƒ½åŠ›ï¼šä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒé¢†åŸŸçŸ¥è¯†
    - å¯æ‰©å±•æ€§ï¼šæ”¯æŒçµæ´»çš„ä¸“å®¶æ•°é‡é…ç½®
    
    åº”ç”¨åœºæ™¯ï¼š
    - å¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆä¸åŒè¯­è¨€ä¸“å®¶ï¼‰
    - å¤šæ¨¡æ€æ¨¡å‹ï¼ˆä¸åŒæ¨¡æ€ä¸“å®¶ï¼‰
    - é¢†åŸŸç‰¹åŒ–æ¨¡å‹ï¼ˆä¸åŒé¢†åŸŸä¸“å®¶ï¼‰
    - è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆä¸‡äº¿å‚æ•°çº§åˆ«ï¼‰
    
    æ€§èƒ½æŒ‡æ ‡ï¼š
    - ä¸“å®¶åˆ©ç”¨ç‡ï¼šè¡¡é‡è´Ÿè½½å¹³è¡¡æ•ˆæœ
    - è·¯ç”±ä¸€è‡´æ€§ï¼šè¯„ä¼°ä¸“å®¶é€‰æ‹©ç¨³å®šæ€§
    - ç¨€ç–åº¦ï¼šæ¿€æ´»ä¸“å®¶å æ€»ä¸“å®¶çš„æ¯”ä¾‹
    """
    
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ–MoEé—¨æ§ç½‘ç»œ
        
        Args:
            config (MiniMindConfig): æ¨¡å‹é…ç½®å‚æ•°
        """
        super().__init__()
        
        # ç¼“å­˜é…ç½®å¯¹è±¡ä»¥ä¾›åç»­ä½¿ç”¨
        # è®­ç»ƒä½œç”¨ï¼šä¿å­˜æ‰€æœ‰MoEç›¸å…³çš„è¶…å‚æ•°é…ç½®
        # æ¨ç†ä½œç”¨ï¼šç¡®ä¿æ¨ç†æ—¶ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹é…ç½®
        self.config = config
        
        # è®¾ç½®top-kå‚æ•°ï¼šæ¯ä¸ªtokené€‰æ‹©æ¿€æ´»çš„ä¸“å®¶æ•°é‡
        # è®­ç»ƒä½œç”¨ï¼šæ§åˆ¶è®­ç»ƒæ—¶çš„ç¨€ç–åº¦å’Œè®¡ç®—å¤æ‚åº¦
        # æ¨ç†ä½œç”¨ï¼šå†³å®šæ¨ç†æ—¶æ¿€æ´»ä¸“å®¶çš„æ•°é‡ï¼Œå½±å“è®¡ç®—æ•ˆç‡
        # å…¸å‹å€¼ï¼š2-4ä¸ªä¸“å®¶ï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡
        self.top_k = config.num_experts_per_tok
        
        # è®¾ç½®è·¯ç”±ä¸“å®¶æ€»æ•°ï¼šå‚ä¸åŠ¨æ€é€‰æ‹©çš„ä¸“å®¶æ•°é‡
        # è®­ç»ƒä½œç”¨ï¼šå†³å®šæ¨¡å‹å®¹é‡å’Œä¸“å®¶ä¸“ä¸šåŒ–ç¨‹åº¦
        # æ¨ç†ä½œç”¨ï¼šå½±å“ä¸“å®¶é€‰æ‹©çš„æœç´¢ç©ºé—´
        # å…¸å‹å€¼ï¼š8-64ä¸ªä¸“å®¶ï¼Œæ ¹æ®æ¨¡å‹è§„æ¨¡è°ƒæ•´
        self.n_routed_experts = config.n_routed_experts

        # è®¾ç½®è¯„åˆ†å‡½æ•°ç±»å‹ï¼šä¸“å®¶é€‰æ‹©çš„æ¦‚ç‡è®¡ç®—æ–¹å¼
        # è®­ç»ƒä½œç”¨ï¼šå½±å“ä¸“å®¶é€‰æ‹©çš„åˆ†å¸ƒç‰¹æ€§å’Œæ¢¯åº¦æµ
        # æ¨ç†ä½œç”¨ï¼šå†³å®šä¸“å®¶æƒé‡çš„è®¡ç®—æ–¹æ³•
        # æ”¯æŒçš„å‡½æ•°ï¼š'softmax'ï¼ˆé»˜è®¤ï¼‰
        self.scoring_func = config.scoring_func
        
        # è®¾ç½®è¾…åŠ©æŸå¤±æƒé‡ï¼šå¹³è¡¡ä¸“å®¶è´Ÿè½½çš„æŸå¤±å‡½æ•°ç³»æ•°
        # è®­ç»ƒä½œç”¨ï¼šé˜²æ­¢ä¸“å®¶ä½¿ç”¨ä¸å‡è¡¡ï¼Œç¡®ä¿æ‰€æœ‰ä¸“å®¶å¾—åˆ°è®­ç»ƒ
        # æ¨ç†ä½œç”¨ï¼šæ¨ç†æ—¶ä¸æ¶‰åŠæŸå¤±è®¡ç®—
        # å…¸å‹å€¼ï¼š0.1-1.0ï¼Œéœ€è¦ä¸ä¸»æŸå¤±å¹³è¡¡
        self.alpha = config.aux_loss_alpha
        
        # è®¾ç½®åºåˆ—çº§è¾…åŠ©æŸå¤±æ ‡å¿—
        # è®­ç»ƒä½œç”¨ï¼šé€‰æ‹©åºåˆ—çº§æˆ–tokençº§çš„è´Ÿè½½å¹³è¡¡ç­–ç•¥
        # æ¨ç†ä½œç”¨ï¼šä¸å½±å“æ¨ç†è¿‡ç¨‹
        self.seq_aux = config.seq_aux

        # è®¾ç½®top-kæ¦‚ç‡å½’ä¸€åŒ–æ ‡å¿—
        # è®­ç»ƒä½œç”¨ï¼šå†³å®šæ˜¯å¦å¯¹é€‰ä¸­ä¸“å®¶çš„æƒé‡è¿›è¡Œå½’ä¸€åŒ–
        # æ¨ç†ä½œç”¨ï¼šå½±å“ä¸“å®¶è¾“å‡ºçš„åŠ æƒæ–¹å¼
        # Trueæ—¶ï¼šç¡®ä¿é€‰ä¸­ä¸“å®¶æƒé‡å’Œä¸º1
        self.norm_topk_prob = config.norm_topk_prob
        
        # è®¾ç½®é—¨æ§è¾“å…¥ç»´åº¦ï¼šé€šå¸¸ç­‰äºæ¨¡å‹éšè—ç»´åº¦
        # è®­ç»ƒä½œç”¨ï¼šç¡®å®šé—¨æ§ç½‘ç»œè¾“å…¥çš„ç‰¹å¾ç»´åº¦
        # æ¨ç†ä½œç”¨ï¼šç¡®ä¿è¾“å…¥å¼ é‡ç»´åº¦åŒ¹é…
        self.gating_dim = config.hidden_size
        
        # åˆ›å»ºé—¨æ§æƒé‡çŸ©é˜µï¼š[n_experts, hidden_size]
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ æ¯ä¸ªä¸“å®¶å¯¹ä¸åŒè¾“å…¥ç‰¹å¾çš„äº²å’Œåº¦
        # æ¨ç†ä½œç”¨ï¼šè®¡ç®—è¾“å…¥ä¸å„ä¸“å®¶çš„åŒ¹é…åˆ†æ•°
        # Parameterï¼šæ³¨å†Œä¸ºå¯è®­ç»ƒå‚æ•°ï¼Œä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°
        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim)))
        
        # åˆå§‹åŒ–æƒé‡å‚æ•°
        # è®­ç»ƒä½œç”¨ï¼šä¸ºæƒé‡çŸ©é˜µè®¾ç½®åˆé€‚çš„åˆå§‹å€¼ï¼Œæœ‰åŠ©äºè®­ç»ƒæ”¶æ•›
        # æ¨ç†ä½œç”¨ï¼šç¡®ä¿æ¨¡å‹å¼€å§‹æ—¶æœ‰åˆç†çš„ä¸“å®¶é€‰æ‹©è¡Œä¸º
        self.reset_parameters()

    def reset_parameters(self) -> None:
        """
        åˆå§‹åŒ–é—¨æ§ç½‘ç»œçš„å‚æ•°
        
        ä½¿ç”¨Kaimingå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡çŸ©é˜µ
        """
        import torch.nn.init as init
        # ä½¿ç”¨Kaimingå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡çŸ©é˜µ
        # è®­ç»ƒä½œç”¨ï¼šä¸ºæƒé‡æä¾›åˆé€‚çš„åˆå§‹åˆ†å¸ƒï¼Œæœ‰åŠ©äºæ¢¯åº¦æµå’Œæ”¶æ•›
        # æ¨ç†ä½œç”¨ï¼šç¡®ä¿æ¨¡å‹åœ¨æœªè®­ç»ƒæ—¶ä¹Ÿæœ‰åˆç†çš„ä¸“å®¶é€‰æ‹©è¡Œä¸º
        # a=math.sqrt(5)ï¼šKaimingåˆå§‹åŒ–çš„æ ‡å‡†å‚æ•°ï¼Œé€‚åˆçº¿æ€§å±‚
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, hidden_states):
        """
        é—¨æ§ç½‘ç»œçš„å‰å‘ä¼ æ’­
        
        ä¸ºæ¯ä¸ªtokenè®¡ç®—ä¸“å®¶é€‰æ‹©æ¦‚ç‡ï¼Œå¹¶é€‰æ‹©top-kä¸ªä¸“å®¶
        
        Args:
            hidden_states (torch.Tensor): è¾“å…¥éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, hidden_size)
            
        Returns:
            tuple: (topk_idx, topk_weight, aux_loss) - é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•ã€æƒé‡å’Œè¾…åŠ©æŸå¤±
        """
        # è·å–è¾“å…¥å¼ é‡çš„å½¢çŠ¶ä¿¡æ¯
        # è®­ç»ƒä½œç”¨ï¼šåŠ¨æ€é€‚åº”ä¸åŒçš„æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦
        # æ¨ç†ä½œç”¨ï¼šå¤„ç†å¯å˜é•¿åº¦çš„è¾“å…¥åºåˆ—
        bsz, seq_len, h = hidden_states.shape
        
        # å±•å¹³è¾“å…¥å¼ é‡ä»¥ä¾¿è¿›è¡Œçº¿æ€§å˜æ¢
        # è®­ç»ƒä½œç”¨ï¼šå°†ä¸‰ç»´å¼ é‡è½¬æ¢ä¸ºäºŒç»´ï¼Œæ–¹ä¾¿çŸ©é˜µä¹˜æ³•è®¡ç®—
        # æ¨ç†ä½œç”¨ï¼šä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œå‡å°‘å¼ é‡æ“ä½œå¤æ‚åº¦
        hidden_states = hidden_states.view(-1, h)  # [batch_size * seq_len, hidden_size]
        
        # === è®¡ç®—ä¸“å®¶é—¨æ§åˆ†æ•° ===
        # é€šè¿‡çº¿æ€§å˜æ¢è®¡ç®—æ¯ä¸ªtokenå¯¹å„ä¸“å®¶çš„äº²å’Œåº¦
        # è®­ç»ƒä½œç”¨ï¼šå­¦ä¹ tokenç‰¹å¾ä¸ä¸“å®¶ç‰¹é•¿çš„åŒ¹é…å…³ç³»
        # æ¨ç†ä½œç”¨ï¼šä¸ºä¸“å®¶é€‰æ‹©æä¾›é‡åŒ–çš„åŒ¹é…åˆ†æ•°
        # logitså½¢çŠ¶ï¼š[batch_size * seq_len, n_routed_experts]
        logits = F.linear(hidden_states, self.weight, None)
        
        # === åº”ç”¨è¯„åˆ†å‡½æ•°è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ ===
        if self.scoring_func == 'softmax':
            # ä½¿ç”¨softmaxå°†logitsè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
            # è®­ç»ƒä½œç”¨ï¼šç¡®ä¿ä¸“å®¶é€‰æ‹©æ¦‚ç‡å’Œä¸º1ï¼Œä¾¿äºæ¢¯åº¦è®¡ç®—
            # æ¨ç†ä½œç”¨ï¼šç”Ÿæˆè§„èŒƒåŒ–çš„ä¸“å®¶é€‰æ‹©æ¦‚ç‡
            scores = logits.softmax(dim=-1)
        else:
            # æŠ›å‡ºå¼‚å¸¸ï¼šç›®å‰ä»…æ”¯æŒsoftmaxè¯„åˆ†å‡½æ•°
            raise NotImplementedError(f'insupportable scoring function for MoE gating: {self.scoring_func}')

        # === é€‰æ‹©top-kä¸ªä¸“å®¶ ===
        # ä¸ºæ¯ä¸ªtokené€‰æ‹©æ¦‚ç‡æœ€é«˜çš„kä¸ªä¸“å®¶
        # è®­ç»ƒä½œç”¨ï¼šå®ç°ç¨€ç–æ¿€æ´»ï¼Œå‡å°‘è®¡ç®—é‡åŒæ—¶ä¿æŒæ¨¡å‹å®¹é‡
        # æ¨ç†ä½œç”¨ï¼šæ˜¾è‘—é™ä½æ¨ç†æˆæœ¬ï¼Œä»…æ¿€æ´»ç›¸å…³ä¸“å®¶
        # sorted=Falseï¼šä¸éœ€è¦æŒ‰æ¦‚ç‡æ’åºï¼Œæé«˜è®¡ç®—æ•ˆç‡
        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)

        # === å¯é€‰çš„top-kæƒé‡å½’ä¸€åŒ– ===
        if self.top_k > 1 and self.norm_topk_prob:
            # å°†é€‰ä¸­ä¸“å®¶çš„æƒé‡é‡æ–°å½’ä¸€åŒ–ï¼Œç¡®ä¿æƒé‡å’Œä¸º1
            # è®­ç»ƒä½œç”¨ï¼šæ ‡å‡†åŒ–æƒé‡åˆ†å¸ƒï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹
            # æ¨ç†ä½œç”¨ï¼šç¡®ä¿ä¸“å®¶è¾“å‡ºçš„åŠ æƒèåˆæ˜¯è§„èŒƒåŒ–çš„
            # 1e-20ï¼šé˜²æ­¢é™¤é›¶é”™è¯¯çš„å°å¸¸æ•°
            denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20
            topk_weight = topk_weight / denominator

        # === è®¡ç®—è¾…åŠ©æŸå¤±ï¼ˆä¸“å®¶è´Ÿè½½å¹³è¡¡ï¼‰ ===
        if self.training and self.alpha > 0.0:
            # ä»…åœ¨è®­ç»ƒæ—¶è®¡ç®—è¾…åŠ©æŸå¤±ï¼Œç”¨äºä¸“å®¶è´Ÿè½½å¹³è¡¡
            scores_for_aux = scores
            aux_topk = self.top_k
            # é‡å¡‘ç´¢å¼•å¼ é‡ä»¥ä¾¿è®¡ç®—è´Ÿè½½å¹³è¡¡
            topk_idx_for_aux_loss = topk_idx.view(bsz, -1)
            
            if self.seq_aux:
                # === åºåˆ—çº§è¾…åŠ©æŸå¤± ===
                # åœ¨åºåˆ—çº§åˆ«è®¡ç®—ä¸“å®¶è´Ÿè½½å¹³è¡¡
                # è®­ç»ƒä½œç”¨ï¼šç¡®ä¿æ¯ä¸ªåºåˆ—ä¸­çš„ä¸“å®¶ä½¿ç”¨ç›¸å¯¹å‡è¡¡
                # æ¨ç†ä½œç”¨ï¼šä¸å‚ä¸æ¨ç†è®¡ç®—
                scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1)
                
                # åˆ›å»ºä¸“å®¶ä½¿ç”¨è®¡æ•°å¼ é‡
                ce = torch.zeros(bsz, self.n_routed_experts, device=hidden_states.device)
                
                # ç»Ÿè®¡æ¯ä¸ªä¸“å®¶è¢«é€‰æ‹©çš„æ¬¡æ•°å¹¶å½’ä¸€åŒ–
                ce.scatter_add_(1, topk_idx_for_aux_loss,
                                torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device)).div_(
                    seq_len * aux_topk / self.n_routed_experts)
                
                # è®¡ç®—åºåˆ—çº§è¾…åŠ©æŸå¤±ï¼šä¸“å®¶ä½¿ç”¨é¢‘ç‡ Ã— ä¸“å®¶é€‰æ‹©æ¦‚ç‡
                aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean() * self.alpha
            else:
                # === æ ‡å‡†è¾…åŠ©æŸå¤± ===
                # åœ¨å…¨å±€çº§åˆ«è®¡ç®—ä¸“å®¶è´Ÿè½½å¹³è¡¡
                # è®­ç»ƒä½œç”¨ï¼šç¡®ä¿æ‰€æœ‰ä¸“å®¶åœ¨æ•´ä¸ªæ‰¹æ¬¡ä¸­å¾—åˆ°å‡è¡¡ä½¿ç”¨
                # æ¨ç†ä½œç”¨ï¼šä¸å‚ä¸æ¨ç†è®¡ç®—
                
                # åˆ›å»ºone-hotç¼–ç è¡¨ç¤ºä¸“å®¶é€‰æ‹©
                mask_ce = F.one_hot(topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts)
                
                # è®¡ç®—ä¸“å®¶çš„å®é™…ä½¿ç”¨é¢‘ç‡
                ce = mask_ce.float().mean(0)
                
                # è®¡ç®—ä¸“å®¶çš„æœŸæœ›é€‰æ‹©æ¦‚ç‡
                Pi = scores_for_aux.mean(0)
                
                # è®¡ç®—è´Ÿè½½å¹³è¡¡å› å­
                fi = ce * self.n_routed_experts
                
                # è¾…åŠ©æŸå¤±ï¼šä½¿ç”¨é¢‘ç‡ä¸æœŸæœ›æ¦‚ç‡çš„ç‚¹ç§¯
                aux_loss = (Pi * fi).sum() * self.alpha
        else:
            # éè®­ç»ƒæ¨¡å¼æˆ–alpha=0æ—¶ï¼Œä¸è®¡ç®—è¾…åŠ©æŸå¤±
            aux_loss = 0
            
        return topk_idx, topk_weight, aux_loss


class MOEFeedForward(nn.Module):
    """
    ä¸“å®¶æ··åˆå‰é¦ˆç½‘ç»œï¼ˆMixture of Experts Feed-Forward Networkï¼‰
    ========================================================
    
    é¡¹ç›®ä¸­çš„ä½œç”¨ï¼š
    - ä½œä¸ºMiniMindæ¨¡å‹çš„é«˜çº§å‰é¦ˆç»„ä»¶ï¼Œæ›¿ä»£æ ‡å‡†FFNå®ç°ç¨€ç–æ¿€æ´»
    - é€šè¿‡å¤šä¸“å®¶æ¶æ„å¤§å¹…æå‡æ¨¡å‹å®¹é‡è€Œä¸æˆæ¯”ä¾‹å¢åŠ è®¡ç®—æˆæœ¬
    - å®ç°åŠ¨æ€ä¸“å®¶é€‰æ‹©å’Œè´Ÿè½½å¹³è¡¡ï¼Œä¼˜åŒ–è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½
    
    å¤§æ¨¡å‹æ¡†æ¶ä¸­çš„ä½œç”¨ï¼š
    - ä»£è¡¨ç°ä»£å¤§æ¨¡å‹çš„å‰æ²¿æ¶æ„è®¾è®¡ï¼Œå®ç°å‚æ•°å’Œè®¡ç®—çš„é«˜æ•ˆè§£è€¦
    - æ”¯æŒä¸‡äº¿å‚æ•°çº§åˆ«æ¨¡å‹çš„å¯è¡Œå®ç°è·¯å¾„
    - æä¾›ä¸“å®¶ä¸“ä¸šåŒ–æœºåˆ¶ï¼Œå¢å¼ºæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„èƒ½åŠ›
    - ä¸ºåˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†æä¾›å¤©ç„¶çš„å¹¶è¡ŒåŒ–æ”¯æŒ
    
    æ¶æ„è®¾è®¡ï¼š
    1. å¤šä¸“å®¶ç³»ç»Ÿï¼š
       - å¤šä¸ªç‹¬ç«‹çš„FeedForwardä¸“å®¶ç½‘ç»œ
       - æ¯ä¸ªä¸“å®¶å…·æœ‰ç›¸åŒçš„æ¶æ„ä½†ç‹¬ç«‹çš„å‚æ•°
       - æ”¯æŒä¸“å®¶çš„ä¸“ä¸šåŒ–å­¦ä¹ å’Œä»»åŠ¡åˆ†å·¥
    
    2. æ™ºèƒ½é—¨æ§æœºåˆ¶ï¼š
       - é›†æˆMoEGateè¿›è¡Œä¸“å®¶é€‰æ‹©
       - æ”¯æŒtop-kè·¯ç”±ç­–ç•¥å’Œæƒé‡åˆ†é…
       - å®ç°è´Ÿè½½å¹³è¡¡å’Œä¸“å®¶åˆ©ç”¨ç‡ä¼˜åŒ–
    
    3. æ··åˆæ¿€æ´»ç­–ç•¥ï¼š
       - è·¯ç”±ä¸“å®¶ï¼šåŠ¨æ€é€‰æ‹©æ¿€æ´»
       - å…±äº«ä¸“å®¶ï¼šå§‹ç»ˆå‚ä¸è®¡ç®—
       - çµæ´»çš„ä¸“å®¶ç»„åˆæœºåˆ¶
    
    è®¡ç®—ä¼˜åŒ–ï¼š
    1. è®­ç»ƒæ—¶ä¼˜åŒ–ï¼š
       - å¹¶è¡Œè®¡ç®—æ‰€æœ‰é€‰ä¸­ä¸“å®¶
       - æƒé‡èšåˆå’Œæ¢¯åº¦åä¼ 
       - æ”¯æŒå¤§æ‰¹é‡è®­ç»ƒ
    
    2. æ¨ç†æ—¶ä¼˜åŒ–ï¼š
       - æ‰¹é‡å¤„ç†ç›¸åŒä¸“å®¶çš„token
       - å‡å°‘å†…å­˜ç¢ç‰‡å’Œè®¡ç®—å¼€é”€
       - ä¸“é—¨çš„æ¨ç†ä¼˜åŒ–è·¯å¾„
    
    æŠ€æœ¯ç‰¹æ€§ï¼š
    - ç¨€ç–æ¿€æ´»ï¼šä»…æ¿€æ´»éƒ¨åˆ†ä¸“å®¶ï¼Œé™ä½è®¡ç®—æˆæœ¬
    - å®¹é‡æ‰©å±•ï¼šæ”¯æŒå¤§é‡ä¸“å®¶ï¼Œæ˜¾è‘—å¢åŠ æ¨¡å‹å‚æ•°
    - ä¸“ä¸šåŒ–å­¦ä¹ ï¼šä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒçŸ¥è¯†é¢†åŸŸ
    - è´Ÿè½½å¹³è¡¡ï¼šç¡®ä¿ä¸“å®¶è®­ç»ƒçš„å‡åŒ€æ€§
    
    åº”ç”¨ä¼˜åŠ¿ï¼š
    - æ¨¡å‹å®¹é‡ï¼šç›¸åŒè®¡ç®—æˆæœ¬ä¸‹è·å¾—æ›´å¤§æ¨¡å‹å®¹é‡
    - æ¨ç†æ•ˆç‡ï¼šç¨€ç–æ¿€æ´»é™ä½å®é™…è®¡ç®—éœ€æ±‚
    - ä¸“ä¸šèƒ½åŠ›ï¼šä¸åŒä¸“å®¶å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
    - å¯æ‰©å±•æ€§ï¼šæ”¯æŒçµæ´»çš„ä¸“å®¶æ•°é‡é…ç½®
    
    é€‚ç”¨åœºæ™¯ï¼š
    - è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆGPT-4ã€PaLMç­‰ï¼‰
    - å¤šè¯­è¨€å’Œå¤šæ¨¡æ€æ¨¡å‹
    - éœ€è¦ä¸“ä¸šåŒ–èƒ½åŠ›çš„é¢†åŸŸæ¨¡å‹
    - è®¡ç®—èµ„æºå—é™ä½†éœ€è¦å¤§å®¹é‡çš„åœºæ™¯
    """
    
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ–MoEå‰é¦ˆç½‘ç»œ
        
        Args:
            config (MiniMindConfig): æ¨¡å‹é…ç½®å‚æ•°
        """
        super().__init__()
        self.config = config
        
        # åˆ›å»ºå¤šä¸ªä¸“å®¶å‰é¦ˆç½‘ç»œ
        self.experts = nn.ModuleList([
            FeedForward(config)
            for _ in range(config.n_routed_experts)
        ])
        
        # é—¨æ§ç½‘ç»œç”¨äºä¸“å®¶é€‰æ‹©
        self.gate = MoEGate(config)
        
        # å¯é€‰çš„å…±äº«ä¸“å®¶ç½‘ç»œï¼ˆå§‹ç»ˆæ¿€æ´»ï¼‰
        if config.n_shared_experts > 0:
            self.shared_experts = nn.ModuleList([
                FeedForward(config)
                for _ in range(config.n_shared_experts)
            ])

    def forward(self, x):
        """
        MoEå‰é¦ˆç½‘ç»œçš„å‰å‘ä¼ æ’­
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, hidden_size)
            
        Returns:
            torch.Tensor: è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        """
        identity = x  # ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºå…±äº«ä¸“å®¶
        orig_shape = x.shape
        bsz, seq_len, _ = x.shape
        
        # ä½¿ç”¨é—¨æ§æœºåˆ¶é€‰æ‹©ä¸“å®¶
        topk_idx, topk_weight, aux_loss = self.gate(x)
        x = x.view(-1, x.shape[-1])  # å±•å¹³ä¸º (batch_size * seq_len, hidden_size)
        flat_topk_idx = topk_idx.view(-1)
        
        if self.training:
            # è®­ç»ƒæ—¶çš„å¤„ç†æ–¹å¼ï¼šç›´æ¥å¹¶è¡Œè®¡ç®—æ‰€æœ‰é€‰ä¸­çš„ä¸“å®¶
            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0)
            y = torch.empty_like(x, dtype=torch.float16)
            
            # ä¸ºæ¯ä¸ªä¸“å®¶è®¡ç®—è¾“å‡º
            for i, expert in enumerate(self.experts):
                mask = (flat_topk_idx == i)
                if mask.any():
                    y[mask] = expert(x[mask]).to(y.dtype)  # ç¡®ä¿ç±»å‹ä¸€è‡´
            
            # æ ¹æ®æƒé‡èšåˆä¸“å®¶è¾“å‡º
            y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1)
            y = y.view(*orig_shape)
        else:
            # æ¨ç†æ—¶çš„ä¼˜åŒ–å¤„ç†æ–¹å¼
            y = self.moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape)
        
        # æ·»åŠ å…±äº«ä¸“å®¶çš„è¾“å‡º
        if self.config.n_shared_experts > 0:
            for expert in self.shared_experts:
                y = y + expert(identity)
        
        # ä¿å­˜è¾…åŠ©æŸå¤±ä¾›åç»­ä½¿ç”¨
        self.aux_loss = aux_loss
        return y

    @torch.no_grad()
    def moe_infer(self, x, flat_expert_indices, flat_expert_weights):
        """
        æ¨ç†æ—¶çš„MoEè®¡ç®—ä¼˜åŒ–ç‰ˆæœ¬
        
        é€šè¿‡æ‰¹é‡å¤„ç†ç›¸åŒä¸“å®¶çš„tokenæ¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œé¿å…äº†è®­ç»ƒæ—¶çš„é‡å¤è®¡ç®—ã€‚
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (total_tokens, hidden_size)
            flat_expert_indices (torch.Tensor): ä¸“å®¶ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º (total_tokens * num_experts_per_tok,)
            flat_expert_weights (torch.Tensor): ä¸“å®¶æƒé‡ï¼Œå½¢çŠ¶ä¸º (total_tokens * num_experts_per_tok, 1)
            
        Returns:
            torch.Tensor: ä¸“å®¶è¾“å‡ºçš„åŠ æƒç»„åˆ
        """
        expert_cache = torch.zeros_like(x)
        
        # æŒ‰ä¸“å®¶ç´¢å¼•æ’åºä»¥ä¾¿æ‰¹é‡å¤„ç†
        idxs = flat_expert_indices.argsort()
        
        # è®¡ç®—æ¯ä¸ªä¸“å®¶å¤„ç†çš„tokenæ•°é‡çš„ç´¯ç§¯å’Œ
        tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0)
        
        # è·å–åŸå§‹tokenç´¢å¼•ï¼ˆè€ƒè™‘åˆ°æ¯ä¸ªtokenå¯èƒ½è¢«å¤šä¸ªä¸“å®¶å¤„ç†ï¼‰
        token_idxs = idxs // self.config.num_experts_per_tok
        
        # ç¤ºä¾‹è¯´æ˜ï¼š
        # å½“tokens_per_expert = [6, 15, 20, 26]ï¼Œä¸“å®¶æ•°é‡ä¸º4
        # ä¸”token_idxs = [3, 7, 19, 21, 24, 25, 4, 5, 6, 10, 11, 12...] æ—¶
        # token_idxs[:6] -> [3, 7, 19, 21, 24, 25] å±äºä¸“å®¶0å¤„ç†çš„token
        # token_idxs[6:15] -> [4, 5, 6, 10, 11, 12...] å±äºä¸“å®¶1å¤„ç†çš„token
        
        # é€ä¸ªä¸“å®¶å¤„ç†
        for i, end_idx in enumerate(tokens_per_expert):
            start_idx = 0 if i == 0 else tokens_per_expert[i - 1]
            if start_idx == end_idx:
                continue  # è·³è¿‡æ²¡æœ‰åˆ†é…tokençš„ä¸“å®¶
            
            expert = self.experts[i]
            exp_token_idx = token_idxs[start_idx:end_idx]
            expert_tokens = x[exp_token_idx]
            
            # è®¡ç®—ä¸“å®¶è¾“å‡ºå¹¶åº”ç”¨æƒé‡
            expert_out = expert(expert_tokens).to(expert_cache.dtype)
            expert_out.mul_(flat_expert_weights[idxs[start_idx:end_idx]])
            
            # å°†åŠ æƒè¾“å‡ºç´¯åŠ åˆ°å¯¹åº”çš„tokenä½ç½®
            expert_cache.scatter_add_(0, exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]), expert_out)

        return expert_cache


class MiniMindBlock(nn.Module):
    """
    MiniMind Transformerå—
    =====================
    
    é¡¹ç›®ä¸­çš„ä½œç”¨ï¼š
    - ä½œä¸ºMiniMindæ¨¡å‹çš„åŸºæœ¬æ„å»ºå•å…ƒï¼Œå®ç°æ ‡å‡†çš„Transformerå±‚æ¶æ„
    - æ•´åˆæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œï¼Œæä¾›å®Œæ•´çš„åºåˆ—å¤„ç†èƒ½åŠ›
    - æ”¯æŒæ¨¡å‹çš„å±‚æ¬¡åŒ–å †å ï¼Œå½¢æˆæ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„
    
    å¤§æ¨¡å‹æ¡†æ¶ä¸­çš„ä½œç”¨ï¼š
    - å®ç°ç°ä»£Transformeræ¶æ„çš„æ ‡å‡†è®¾è®¡æ¨¡å¼
    - æ”¯æŒPre-Normç»“æ„ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§èƒ½
    - æä¾›æ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºæ¨¡å‹çš„å¤ç”¨å’Œæ‰©å±•
    - é›†æˆå…ˆè¿›æŠ€æœ¯ï¼ˆGQAã€MoEã€RoPEç­‰ï¼‰ï¼Œä¿æŒæ¶æ„çš„å‰ç»æ€§
    
    æ¶æ„ç‰¹ç‚¹ï¼š
    1. Pre-Normè®¾è®¡ï¼š
       - åœ¨è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œä¹‹å‰åº”ç”¨RMSNorm
       - ç›¸æ¯”Post-Normå…·æœ‰æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§
       - æ”¯æŒæ›´æ·±å±‚ç½‘ç»œçš„è®­ç»ƒå’Œæ”¶æ•›
    
    2. æ®‹å·®è¿æ¥ï¼š
       - å®ç°æ¢¯åº¦çš„ç›´æ¥ä¼ æ’­è·¯å¾„
       - ç¼“è§£æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
       - ä¿æŒä¿¡æ¯çš„å®Œæ•´æ€§å’ŒæµåŠ¨æ€§
    
    3. æ¨¡å—åŒ–ç»„åˆï¼š
       - è‡ªæ³¨æ„åŠ›å±‚ï¼šå¤„ç†åºåˆ—å†…çš„ä¾èµ–å…³ç³»
       - å‰é¦ˆç½‘ç»œï¼šæä¾›ä½ç½®ç‹¬ç«‹çš„éçº¿æ€§å˜æ¢
       - æ”¯æŒæ ‡å‡†FFNå’ŒMoE FFNçš„çµæ´»åˆ‡æ¢
    
    è®¡ç®—æµç¨‹ï¼š
    1. ç¬¬ä¸€ä¸ªå­å±‚ï¼š
       Input â†’ RMSNorm â†’ Self-Attention â†’ Residual Connection
    
    2. ç¬¬äºŒä¸ªå­å±‚ï¼š
       Hidden â†’ RMSNorm â†’ Feed-Forward â†’ Residual Connection
    
    æŠ€æœ¯ä¼˜åŠ¿ï¼š
    - è®­ç»ƒç¨³å®šæ€§ï¼šPre-Norm + RMSNormçš„ç»„åˆ
    - è®¡ç®—æ•ˆç‡ï¼šRMSNormç›¸æ¯”LayerNormæ›´é«˜æ•ˆ
    - çµæ´»æ€§ï¼šæ”¯æŒä¸åŒç±»å‹çš„å‰é¦ˆç½‘ç»œ
    - å¯æ‰©å±•æ€§ï¼šä¾¿äºæ„å»ºä¸åŒæ·±åº¦çš„æ¨¡å‹
    
    åº”ç”¨ç‰¹æ€§ï¼š
    - æ”¯æŒKVç¼“å­˜çš„å¢é‡è®¡ç®—
    - å…¼å®¹Flash AttentionåŠ é€Ÿ
    - é€‚é…ä¸åŒçš„æ³¨æ„åŠ›æ©ç ç­–ç•¥
    - æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–å†…å­˜
    
    åœ¨æ¨¡å‹ä¸­çš„ä½ç½®ï¼š
    - ä½œä¸ºMiniMindModelçš„åŸºæœ¬ç»„æˆå•å…ƒ
    - é€šè¿‡å¤šå±‚å †å å½¢æˆå®Œæ•´çš„è¯­è¨€æ¨¡å‹
    - æ¯å±‚å¤„ç†ä¸åŒæŠ½è±¡çº§åˆ«çš„è¯­è¨€ç‰¹å¾
    - ä»ä½çº§è¯­æ³•åˆ°é«˜çº§è¯­ä¹‰çš„æ¸è¿›å¼å»ºæ¨¡
    """
    
    def __init__(self, layer_id: int, config: MiniMindConfig):
        """
        åˆå§‹åŒ–Transformerå—
        
        Args:
            layer_id (int): å±‚ç´¢å¼•ï¼Œç”¨äºæ ‡è¯†å½“å‰å±‚
            config (MiniMindConfig): æ¨¡å‹é…ç½®å‚æ•°
        """
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        self.head_dim = config.hidden_size // config.num_attention_heads
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
        self.self_attn = Attention(config)

        self.layer_id = layer_id
        
        # Pre-Normï¼šæ³¨æ„åŠ›å‰çš„å±‚å½’ä¸€åŒ–
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        # Pre-Normï¼šå‰é¦ˆç½‘ç»œå‰çš„å±‚å½’ä¸€åŒ–
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        # å‰é¦ˆç½‘ç»œï¼šæ ¹æ®é…ç½®é€‰æ‹©æ ‡å‡†FFNæˆ–MoE
        self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)

    def forward(self, hidden_states, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):
        """
        Transformerå—çš„å‰å‘ä¼ æ’­
        
        å®ç°æ ‡å‡†çš„Transformeræ¶æ„ï¼š
        1. æ®‹å·®è¿æ¥ + è‡ªæ³¨æ„åŠ›ï¼ˆå¸¦Pre-Normï¼‰
        2. æ®‹å·®è¿æ¥ + å‰é¦ˆç½‘ç»œï¼ˆå¸¦Pre-Normï¼‰
        
        Args:
            hidden_states (torch.Tensor): è¾“å…¥éšè—çŠ¶æ€
            position_embeddings (Tuple): ä½ç½®ç¼–ç  (cos, sin)
            past_key_value (Optional[Tuple]): KVç¼“å­˜
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            attention_mask (Optional[torch.Tensor]): æ³¨æ„åŠ›æ©ç 
            
        Returns:
            Tuple[torch.Tensor, Optional[Tuple]]: (è¾“å‡ºéšè—çŠ¶æ€, æ–°çš„KVç¼“å­˜)
        """
        # ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥ï¼šè‡ªæ³¨æ„åŠ›
        residual = hidden_states
        
        # Pre-Norm + è‡ªæ³¨æ„åŠ›
        hidden_states, present_key_value = self.self_attn(
            self.input_layernorm(hidden_states), 
            position_embeddings,
            past_key_value, 
            use_cache, 
            attention_mask
        )
        
        # æ®‹å·®è¿æ¥
        hidden_states += residual
        
        # ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥ï¼šå‰é¦ˆç½‘ç»œ
        # Pre-Norm + MLP + æ®‹å·®è¿æ¥
        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))
        
        return hidden_states, present_key_value


class MiniMindModel(nn.Module):
    """
    MiniMindä¸»æ¨¡å‹ç±»
    ===============
    
    é¡¹ç›®ä¸­çš„ä½œç”¨ï¼š
    - ä½œä¸ºMiniMindé¡¹ç›®çš„æ ¸å¿ƒæ¨¡å‹æ¶æ„ï¼Œå®ç°å®Œæ•´çš„Transformerè¯­è¨€æ¨¡å‹
    - æ•´åˆè¯åµŒå…¥ã€ä½ç½®ç¼–ç ã€å¤šå±‚Transformerå’Œè¾“å‡ºå½’ä¸€åŒ–
    - æä¾›ç»Ÿä¸€çš„å‰å‘ä¼ æ’­æ¥å£ï¼Œæ”¯æŒè®­ç»ƒå’Œæ¨ç†çš„å…¨æµç¨‹
    
    å¤§æ¨¡å‹æ¡†æ¶ä¸­çš„ä½œç”¨ï¼š
    - å®ç°ç°ä»£è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„æ ‡å‡†æ¶æ„è®¾è®¡
    - é›†æˆå¤šé¡¹å‰æ²¿æŠ€æœ¯ï¼ˆRoPEã€GQAã€MoEç­‰ï¼‰ï¼Œä»£è¡¨å½“å‰æŠ€æœ¯æ°´å¹³
    - æä¾›å¯æ‰©å±•çš„æ¨¡å‹åŸºç¡€ï¼Œæ”¯æŒä¸åŒè§„æ¨¡çš„æ¨¡å‹é…ç½®
    - å…¼å®¹Hugging Faceç”Ÿæ€ï¼Œä¾¿äºæ¨¡å‹çš„åˆ†äº«å’Œéƒ¨ç½²
    
    æ¶æ„ç»„æˆï¼š
    1. è¯åµŒå…¥å±‚ï¼ˆToken Embeddingï¼‰ï¼š
       - å°†ç¦»æ•£tokenè½¬æ¢ä¸ºè¿ç»­å‘é‡è¡¨ç¤º
       - æ”¯æŒ6400è¯æ±‡è¡¨ï¼ˆå¯é…ç½®ï¼‰
       - ä¸è¾“å‡ºå±‚æƒé‡å…±äº«ï¼Œå‡å°‘å‚æ•°é‡
    
    2. ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼š
       - é‡‡ç”¨RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰æŠ€æœ¯
       - é¢„è®¡ç®—å¹¶ç¼“å­˜é¢‘ç‡çŸ©é˜µ
       - æ”¯æŒé•¿åºåˆ—å¤–æ¨å’Œä½ç½®æ„ŸçŸ¥
    
    3. å¤šå±‚Transformerï¼š
       - å¯é…ç½®çš„å±‚æ•°ï¼ˆé€šå¸¸8-32å±‚ï¼‰
       - æ¯å±‚åŒ…å«è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œ
       - æ”¯æŒæ ‡å‡†FFNå’ŒMoEä¸¤ç§æ¨¡å¼
    
    4. è¾“å‡ºå½’ä¸€åŒ–ï¼š
       - æœ€ç»ˆçš„RMSNormå±‚
       - ç¨³å®šè¾“å‡ºåˆ†å¸ƒ
       - ä¸ºåç»­çš„è¯­è¨€å»ºæ¨¡å¤´åšå‡†å¤‡
    
    æ ¸å¿ƒæŠ€æœ¯ç‰¹æ€§ï¼š
    1. RoPEä½ç½®ç¼–ç ï¼š
       - ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œå…·å¤‡å¤–æ¨èƒ½åŠ›
       - æ”¯æŒæ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—
       - é«˜æ•ˆçš„é¢„è®¡ç®—å’Œç¼“å­˜æœºåˆ¶
    
    2. KVç¼“å­˜æ”¯æŒï¼š
       - å¢é‡ç”Ÿæˆçš„å…³é”®ä¼˜åŒ–
       - é¿å…é‡å¤è®¡ç®—å†å²token
       - æ”¯æŒæµå¼ç”Ÿæˆå’Œå¯¹è¯åœºæ™¯
    
    3. æ¨¡å—åŒ–è®¾è®¡ï¼š
       - æ¸…æ™°çš„å±‚æ¬¡ç»“æ„
       - ä¾¿äºè°ƒè¯•å’Œæ€§èƒ½åˆ†æ
       - æ”¯æŒä¸åŒç»„ä»¶çš„ç‹¬ç«‹ä¼˜åŒ–
    
    4. å†…å­˜ä¼˜åŒ–ï¼š
       - æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹
       - å¯é€‰çš„æ··åˆç²¾åº¦è®­ç»ƒ
       - é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—
    
    è®­ç»ƒå’Œæ¨ç†ç‰¹æ€§ï¼š
    - æ”¯æŒå› æœè¯­è¨€å»ºæ¨¡ç›®æ ‡
    - å…¼å®¹åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥
    - æ”¯æŒå¤šç§ç”Ÿæˆç­–ç•¥ï¼ˆè´ªå©ªã€é‡‡æ ·ç­‰ï¼‰
    - æä¾›å®Œæ•´çš„çŠ¶æ€ç®¡ç†æœºåˆ¶
    
    æ€§èƒ½æŒ‡æ ‡ï¼š
    - å‚æ•°é‡ï¼š26M-108Mï¼ˆæ ¹æ®é…ç½®ï¼‰
    - ä¸Šä¸‹æ–‡é•¿åº¦ï¼šæœ€å¤§32K tokens
    - æ¨ç†é€Ÿåº¦ï¼šæ”¯æŒKVç¼“å­˜ä¼˜åŒ–
    - å†…å­˜æ•ˆç‡ï¼šRMSNorm + GQAä¼˜åŒ–
    
    é€‚ç”¨åœºæ™¯ï¼š
    - è½»é‡çº§è¯­è¨€æ¨¡å‹çš„ç ”ç©¶å’Œå¼€å‘
    - æ•™è‚²å’Œå­¦æœ¯ç ”ç©¶é¡¹ç›®
    - èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹éƒ¨ç½²
    - å¤§æ¨¡å‹æŠ€æœ¯çš„åŸå‹éªŒè¯
    """
    
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ–MiniMindæ¨¡å‹
        
        Args:
            config (MiniMindConfig): æ¨¡å‹é…ç½®å‚æ•°
        """
        super().__init__()
        self.config = config
        self.vocab_size, self.num_hidden_layers = config.vocab_size, config.num_hidden_layers
        
        # è¯åµŒå…¥å±‚ï¼šå°†token IDè½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        
        # Dropoutå±‚ç”¨äºæ­£åˆ™åŒ–
        self.dropout = nn.Dropout(config.dropout)
        
        # å¤šä¸ªTransformerå—
        self.layers = nn.ModuleList([MiniMindBlock(l, config) for l in range(self.num_hidden_layers)])
        
        # æœ€ç»ˆçš„å±‚å½’ä¸€åŒ–
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        # é¢„è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç å¹¶æ³¨å†Œä¸ºç¼“å†²åŒºï¼ˆä¸å‚ä¸è®­ç»ƒï¼‰
        freqs_cos, freqs_sin = precompute_freqs_cis(
            dim=config.hidden_size // config.num_attention_heads,
            end=config.max_position_embeddings, 
            theta=config.rope_theta
        )
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                **kwargs):
        """
        æ¨¡å‹çš„å‰å‘ä¼ æ’­
        
        Args:
            input_ids (torch.Tensor): è¾“å…¥token IDï¼Œå½¢çŠ¶ä¸º (batch_size, seq_length)
            attention_mask (Optional[torch.Tensor]): æ³¨æ„åŠ›æ©ç 
            past_key_values (Optional[List]): KVç¼“å­˜åˆ—è¡¨ï¼Œç”¨äºå¢é‡ç”Ÿæˆ
            use_cache (bool): æ˜¯å¦è¿”å›KVç¼“å­˜
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            tuple: (hidden_states, past_key_values, aux_loss)
                - hidden_states: æœ€ç»ˆéšè—çŠ¶æ€
                - past_key_values: æ–°çš„KVç¼“å­˜
                - aux_loss: MoEçš„è¾…åŠ©æŸå¤±
        """
        batch_size, seq_length = input_ids.shape
        
        # åˆå§‹åŒ–æˆ–ä½¿ç”¨ä¼ å…¥çš„KVç¼“å­˜
        past_key_values = past_key_values or [None] * len(self.layers)
        
        # è®¡ç®—èµ·å§‹ä½ç½®ï¼ˆç”¨äºå¢é‡ç”Ÿæˆï¼‰
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0

        # è¯åµŒå…¥ + Dropout
        hidden_states = self.dropout(self.embed_tokens(input_ids))

        # è·å–å½“å‰åºåˆ—çš„ä½ç½®ç¼–ç 
        # ğŸ¯ è¿™é‡Œæ˜¯ position_embeddings çš„è®¡ç®—æºå¤´ï¼
        # ============================================
        # 
        # ğŸ“ ä½ç½®ç¼–ç çš„åˆ‡ç‰‡é€»è¾‘ï¼š
        # --------------------
        # self.freqs_cos å’Œ self.freqs_sin æ˜¯åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶é¢„è®¡ç®—çš„å®Œæ•´ä½ç½®ç¼–ç 
        # å½¢çŠ¶ï¼š[max_position_embeddings, head_dim//2]
        # ä¾‹å¦‚ï¼š[32768, 32] (å‡è®¾head_dim=64)
        # 
        # start_pos: å½“å‰ç”Ÿæˆçš„èµ·å§‹ä½ç½®
        # - è®­ç»ƒæ—¶ï¼šé€šå¸¸ä¸º0 (å®Œæ•´åºåˆ—)
        # - æ¨ç†æ—¶ï¼šç­‰äºå·²ç”Ÿæˆçš„tokenæ•°é‡ (å¢é‡ç”Ÿæˆ)
        # 
        # seq_length: å½“å‰éœ€è¦å¤„ç†çš„åºåˆ—é•¿åº¦
        # - è®­ç»ƒæ—¶ï¼šå®Œæ•´åºåˆ—é•¿åº¦ (å¦‚512)
        # - æ¨ç†æ—¶ï¼šé€šå¸¸ä¸º1 (æ–°ç”Ÿæˆçš„token)
        # 
        # ğŸ”„ åˆ‡ç‰‡ç¤ºä¾‹ï¼š
        # -----------
        # è®­ç»ƒæ—¶ï¼šstart_pos=0, seq_length=512
        # â†’ å– freqs_cos[0:512] å’Œ freqs_sin[0:512]
        # 
        # æ¨ç†ç¬¬1æ­¥ï¼šstart_pos=0, seq_length=1  
        # â†’ å– freqs_cos[0:1] å’Œ freqs_sin[0:1]
        # 
        # æ¨ç†ç¬¬10æ­¥ï¼šstart_pos=9, seq_length=1
        # â†’ å– freqs_cos[9:10] å’Œ freqs_sin[9:10]
        # 
        # ğŸ’¡ ä¸ºä»€ä¹ˆè¦åŠ¨æ€åˆ‡ç‰‡ï¼Ÿ
        # -------------------
        # 1. **å†…å­˜æ•ˆç‡**ï¼šåªå–éœ€è¦çš„éƒ¨åˆ†ï¼Œä¸æµªè´¹å†…å­˜
        # 2. **å¢é‡ç”Ÿæˆ**ï¼šæ”¯æŒé€æ­¥ç”Ÿæˆï¼Œä½ç½®æ­£ç¡®å¯¹åº”
        # 3. **ç¼“å­˜å‹å¥½**ï¼šä¸KVç¼“å­˜çš„ä½ç½®ä¿æŒä¸€è‡´
        # 
        # ğŸš€ æ¥ä¸‹æ¥çš„ä¼ é€’è·¯å¾„ï¼š
        # -------------------
        # è¿™ä¸ª position_embeddings å…ƒç»„ä¼šè¢«ä¼ é€’ç»™æ¯ä¸ª MiniMindBlockï¼Œ
        # å†ä¼ é€’ç»™æ¯ä¸ª Attention å±‚ï¼Œæœ€ç»ˆåœ¨é‚£é‡Œè§£åŒ…ä¸º cos, sin ä½¿ç”¨ï¼
        position_embeddings = (
            self.freqs_cos[start_pos:start_pos + seq_length],
            self.freqs_sin[start_pos:start_pos + seq_length]
        )

        # é€å±‚å‰å‘ä¼ æ’­
        presents = []
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):
            hidden_states, present = layer(
                hidden_states,
                position_embeddings,
                past_key_value=past_key_value,
                use_cache=use_cache,
                attention_mask=attention_mask
            )
            presents.append(present)

        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        hidden_states = self.norm(hidden_states)

        # æ”¶é›†æ‰€æœ‰MoEå±‚çš„è¾…åŠ©æŸå¤±
        aux_loss = sum(
            layer.mlp.aux_loss
            for layer in self.layers
            if isinstance(layer.mlp, MOEFeedForward)
        )

        return hidden_states, presents, aux_loss


class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):
    """
    MiniMindå› æœè¯­è¨€å»ºæ¨¡å®Œæ•´æ¨¡å‹
    ===========================
    
    é¡¹ç›®ä¸­çš„ä½œç”¨ï¼š
    - ä½œä¸ºMiniMindé¡¹ç›®çš„æœ€ç»ˆæ¨¡å‹ç±»ï¼Œæä¾›å®Œæ•´çš„è¯­è¨€å»ºæ¨¡åŠŸèƒ½
    - å°è£…MiniMindModelå¹¶æ·»åŠ è¯­è¨€å»ºæ¨¡å¤´ï¼Œå®ç°ç«¯åˆ°ç«¯çš„æ–‡æœ¬ç”Ÿæˆ
    - ç»§æ‰¿Hugging Faceæ¥å£ï¼Œæä¾›æ ‡å‡†åŒ–çš„æ¨¡å‹APIå’Œç”Ÿæ€å…¼å®¹æ€§
    
    å¤§æ¨¡å‹æ¡†æ¶ä¸­çš„ä½œç”¨ï¼š
    - å®ç°ç°ä»£å› æœè¯­è¨€æ¨¡å‹çš„å®Œæ•´æ¶æ„å’ŒåŠŸèƒ½
    - æä¾›ä¸GPTç³»åˆ—æ¨¡å‹å…¼å®¹çš„æ¥å£å’Œè¡Œä¸ºæ¨¡å¼
    - æ”¯æŒå®Œæ•´çš„æ¨¡å‹ç”Ÿå‘½å‘¨æœŸï¼šè®­ç»ƒã€ä¿å­˜ã€åŠ è½½ã€æ¨ç†ã€éƒ¨ç½²
    - é›†æˆå…ˆè¿›çš„æ–‡æœ¬ç”ŸæˆæŠ€æœ¯ï¼Œæ”¯æŒå¤šç§ç”Ÿæˆç­–ç•¥å’Œä¼˜åŒ–
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å› æœè¯­è¨€å»ºæ¨¡ï¼š
       - åŸºäºå‰æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„ä»»åŠ¡
       - æ”¯æŒè‡ªå›å½’ç”Ÿæˆå’Œåºåˆ—ç»­å†™
       - å®ç°æ ‡å‡†çš„è¯­è¨€æ¨¡å‹ç›®æ ‡å‡½æ•°
    
    2. æ–‡æœ¬ç”Ÿæˆï¼š
       - ç»§æ‰¿GenerationMixinï¼Œæ”¯æŒå¤šç§ç”Ÿæˆç­–ç•¥
       - è´ªå©ªæœç´¢ã€æŸæœç´¢ã€é‡‡æ ·ç”Ÿæˆç­‰
       - æ”¯æŒæ¸©åº¦æ§åˆ¶ã€top-kã€top-pç­‰å‚æ•°
    
    3. æ¨¡å‹ç®¡ç†ï¼š
       - ç»§æ‰¿PreTrainedModelï¼Œå…¼å®¹HFç”Ÿæ€
       - æ”¯æŒæ¨¡å‹çš„ä¿å­˜å’ŒåŠ è½½
       - æä¾›é…ç½®ç®¡ç†å’Œç‰ˆæœ¬æ§åˆ¶
    
    æ¶æ„ç‰¹ç‚¹ï¼š
    1. æƒé‡å…±äº«è®¾è®¡ï¼š
       - è¯åµŒå…¥å±‚å’Œè¾“å‡ºå±‚å…±äº«æƒé‡å‚æ•°
       - å‡å°‘æ¨¡å‹å‚æ•°é‡ï¼Œæå‡è®­ç»ƒæ•ˆç‡
       - å¢å¼ºè¯æ±‡è¡¨ç¤ºçš„ä¸€è‡´æ€§
    
    2. çµæ´»çš„è¾“å‡ºæ§åˆ¶ï¼š
       - æ”¯æŒlogits_to_keepå‚æ•°ä¼˜åŒ–å†…å­˜
       - ä»…è®¡ç®—éœ€è¦çš„è¾“å‡ºä½ç½®
       - é€‚åº”ä¸åŒçš„è®­ç»ƒå’Œæ¨ç†éœ€æ±‚
    
    3. å®Œæ•´çš„çŠ¶æ€ç®¡ç†ï¼š
       - æ”¯æŒKVç¼“å­˜çš„ä¼ é€’å’Œç®¡ç†
       - æä¾›è¾…åŠ©æŸå¤±çš„æ”¶é›†å’Œè¿”å›
       - å…¼å®¹å„ç§è®­ç»ƒå’Œæ¨ç†æ¨¡å¼
    
    Hugging Faceé›†æˆç‰¹æ€§ï¼š
    1. æ ‡å‡†åŒ–æ¥å£ï¼š
       - forwardæ–¹æ³•å…¼å®¹transformersåº“
       - æ”¯æŒæ ‡å‡†çš„è¾“å…¥è¾“å‡ºæ ¼å¼
       - æä¾›å®Œæ•´çš„æ¨¡å‹é…ç½®æ”¯æŒ
    
    2. ç”ŸæˆåŠŸèƒ½ï¼š
       - é›†æˆgenerateæ–¹æ³•å’Œç”Ÿæˆå·¥å…·
       - æ”¯æŒæ‰¹é‡ç”Ÿæˆå’Œæµå¼è¾“å‡º
       - æä¾›ä¸°å¯Œçš„ç”Ÿæˆæ§åˆ¶å‚æ•°
    
    3. æ¨¡å‹æŒä¹…åŒ–ï¼š
       - æ”¯æŒsave_pretrainedå’Œfrom_pretrained
       - å…¼å®¹æ¨¡å‹Hubçš„ä¸Šä¼ å’Œä¸‹è½½
       - æä¾›å®Œæ•´çš„æ¨¡å‹å…ƒæ•°æ®ç®¡ç†
    
    åº”ç”¨åœºæ™¯ï¼š
    1. æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼š
       - åˆ›æ„å†™ä½œå’Œå†…å®¹ç”Ÿæˆ
       - å¯¹è¯ç³»ç»Ÿå’ŒèŠå¤©æœºå™¨äºº
       - ä»£ç ç”Ÿæˆå’ŒæŠ€æœ¯æ–‡æ¡£
    
    2. è¯­è¨€ç†è§£ä»»åŠ¡ï¼š
       - æ–‡æœ¬åˆ†ç±»å’Œæƒ…æ„Ÿåˆ†æ
       - é—®ç­”ç³»ç»Ÿå’Œä¿¡æ¯æŠ½å–
       - æ–‡æœ¬æ‘˜è¦å’Œå…³é”®è¯æå–
    
    3. ç ”ç©¶å’Œå¼€å‘ï¼š
       - è¯­è¨€æ¨¡å‹æŠ€æœ¯ç ”ç©¶
       - æ–°æ¶æ„å’Œç®—æ³•éªŒè¯
       - æ•™è‚²å’Œå­¦ä¹ é¡¹ç›®
    
    æ€§èƒ½ä¼˜åŒ–ï¼š
    - æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜å ç”¨
    - å…¼å®¹æ··åˆç²¾åº¦è®­ç»ƒæå‡é€Ÿåº¦
    - æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†
    - æä¾›KVç¼“å­˜ä¼˜åŒ–æ¨ç†æ€§èƒ½
    
    éƒ¨ç½²æ”¯æŒï¼š
    - æ”¯æŒONNXå¯¼å‡ºå’Œä¼˜åŒ–
    - å…¼å®¹TensorRTç­‰æ¨ç†å¼•æ“
    - æ”¯æŒé‡åŒ–å’Œå‰ªæç­‰å‹ç¼©æŠ€æœ¯
    - æä¾›APIæœåŠ¡å’ŒWebç•Œé¢é›†æˆ
    """
    # config_classè¯¦ç»†è¯´æ˜ï¼š
    # è¿™æ˜¯transformersåº“è¦æ±‚çš„é…ç½®ç±»å…³è”
    # æŒ‡å®šäº†ä¸å½“å‰æ¨¡å‹ç±»é…å¥—ä½¿ç”¨çš„é…ç½®ç±»
    # transformersåº“ä½¿ç”¨æ­¤ä¿¡æ¯è¿›è¡Œè‡ªåŠ¨é…ç½®ç®¡ç†ï¼š
    # - åœ¨save_pretrained()æ—¶è‡ªåŠ¨ä¿å­˜æ­£ç¡®çš„é…ç½®ç±»
    # - åœ¨from_pretrained()æ—¶è‡ªåŠ¨åŠ è½½å¯¹åº”çš„é…ç½®
    # - åœ¨AutoModelä¸­å®ç°é…ç½®å’Œæ¨¡å‹çš„è‡ªåŠ¨åŒ¹é…
    config_class = MiniMindConfig

    def __init__(self, config: MiniMindConfig = None):
        """
        åˆå§‹åŒ–å› æœè¯­è¨€æ¨¡å‹
        
        Args:
            config (MiniMindConfig): æ¨¡å‹é…ç½®å‚æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or MiniMindConfig()
        super().__init__(self.config)
        
        # ä¸»æ¨¡å‹
        self.model = MiniMindModel(self.config)
        
        # è¯­è¨€å»ºæ¨¡å¤´ï¼šå°†éšè—çŠ¶æ€è½¬æ¢ä¸ºè¯æ±‡è¡¨ä¸Šçš„logits
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)
        
        # æƒé‡å…±äº«ï¼šè¯åµŒå…¥å±‚å’Œè¾“å‡ºå±‚å…±äº«å‚æ•°ï¼ˆå‡å°‘å‚æ•°é‡ï¼Œæå‡æ€§èƒ½ï¼‰
        self.model.embed_tokens.weight = self.lm_head.weight
        
        # CausalLMOutputWithPastè¯¦ç»†è¯´æ˜ï¼š
        # è¿™æ˜¯transformersåº“å®šä¹‰çš„æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ç±»ï¼Œç”¨äºå› æœè¯­è¨€æ¨¡å‹
        # ä¸»è¦åŒ…å«ä»¥ä¸‹å±æ€§ï¼š
        # - logits: æ¨¡å‹é¢„æµ‹çš„è¯æ±‡è¡¨æ¦‚ç‡åˆ†å¸ƒï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len, vocab_size)
        # - past_key_values: KVç¼“å­˜ï¼Œç”¨äºåŠ é€Ÿåºåˆ—ç”Ÿæˆï¼Œé¿å…é‡å¤è®¡ç®—
        # - hidden_states: å„å±‚çš„éšè—çŠ¶æ€ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºåˆ†æå’Œè°ƒè¯•
        # - attentions: æ³¨æ„åŠ›æƒé‡ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºå¯è§†åŒ–å’Œåˆ†æ
        # - loss: è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆåœ¨è®­ç»ƒæ—¶è®¡ç®—ï¼‰
        # ä½¿ç”¨æ ‡å‡†æ ¼å¼ç¡®ä¿ä¸Hugging Faceç”Ÿæ€ç³»ç»Ÿçš„å®Œå…¨å…¼å®¹
        self.OUT = CausalLMOutputWithPast()  # è¾“å‡ºæ ¼å¼å®¹å™¨ï¼Œå…¼å®¹Hugging Faceæ ‡å‡†

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                logits_to_keep: Union[int, torch.Tensor] = 0,
                **args):
        """
        æ¨¡å‹çš„å‰å‘ä¼ æ’­
        
        Args:
            input_ids (torch.Tensor): è¾“å…¥token IDï¼Œå½¢çŠ¶ä¸º (batch_size, seq_length)
            attention_mask (Optional[torch.Tensor]): æ³¨æ„åŠ›æ©ç 
            past_key_values (Optional[List]): KVç¼“å­˜ï¼Œç”¨äºå¢é‡ç”Ÿæˆ
            use_cache (bool): æ˜¯å¦è¿”å›KVç¼“å­˜
            logits_to_keep (Union[int, torch.Tensor]): ä¿ç•™çš„logitsæ•°é‡ï¼Œç”¨äºä¼˜åŒ–å†…å­˜
            **args: å…¶ä»–å‚æ•°
            
        Returns:
            CausalLMOutputWithPast: åŒ…å«logitsã€éšè—çŠ¶æ€ã€KVç¼“å­˜å’Œè¾…åŠ©æŸå¤±çš„è¾“å‡ºå¯¹è±¡
        """
        # ä¸»æ¨¡å‹å‰å‘ä¼ æ’­
        h, past_kvs, aux_loss = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **args
        )
        
        # è®¡ç®—logitsï¼ˆä»…ä¿ç•™éœ€è¦çš„éƒ¨åˆ†ä»¥èŠ‚çœå†…å­˜ï¼‰
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(h[:, slice_indices, :])
        
        # å°è£…è¾“å‡ºåˆ°æ ‡å‡†æ ¼å¼
        # ä½¿ç”¨CausalLMOutputWithPastçš„__setitem__æ–¹æ³•è®¾ç½®å„ä¸ªå±æ€§
        # è¿™ç§æ–¹å¼ç¡®ä¿è¾“å‡ºæ ¼å¼å®Œå…¨å…¼å®¹transformersåº“çš„æ ‡å‡†
        self.OUT.__setitem__('last_hidden_state', h)          # æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
        self.OUT.__setitem__('logits', logits)                # è¯æ±‡è¡¨é¢„æµ‹æ¦‚ç‡
        self.OUT.__setitem__('aux_loss', aux_loss)            # MoEè¾…åŠ©æŸå¤±ï¼ˆå¦‚æœä½¿ç”¨MoEï¼‰
        self.OUT.__setitem__('past_key_values', past_kvs)     # KVç¼“å­˜ï¼Œç”¨äºå¢é‡ç”Ÿæˆ
        
        # è¿”å›çš„è¾“å‡ºå¯¹è±¡å¯ä»¥åƒå­—å…¸ä¸€æ ·è®¿é—®ï¼š
        # output.logits, output.past_key_values, output.hidden_statesç­‰
        # ä¹Ÿå¯ä»¥åƒå…ƒç»„ä¸€æ ·è§£åŒ…ï¼šloss, logits, past_key_values = output
        return self.OUT


# =============================================================================
# Transformersåº“ä½¿ç”¨ç¤ºä¾‹
# =============================================================================

"""
ä»¥ä¸‹æ˜¯MiniMindæ¨¡å‹ä¸transformersåº“é›†æˆçš„å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ï¼š

1. æ¨¡å‹ä¿å­˜ç¤ºä¾‹ï¼š
```python
from model.model_minimind import MiniMindForCausalLM, MiniMindConfig

# åˆ›å»ºé…ç½®
config = MiniMindConfig(
    hidden_size=512,
    num_attention_heads=8,
    num_hidden_layers=8,
    vocab_size=6400
)

# åˆ›å»ºæ¨¡å‹
model = MiniMindForCausalLM(config)

# ä¿å­˜æ¨¡å‹å’Œé…ç½®ï¼ˆtransformersæ ‡å‡†æ ¼å¼ï¼‰
model.save_pretrained("./minimind_model")
# è¿™ä¼šä¿å­˜ï¼š
# - config.json: æ¨¡å‹é…ç½®æ–‡ä»¶
# - pytorch_model.bin: æ¨¡å‹æƒé‡æ–‡ä»¶
# - generation_config.json: ç”Ÿæˆé…ç½®æ–‡ä»¶
```

2. æ¨¡å‹åŠ è½½ç¤ºä¾‹ï¼š
```python
# ä»ä¿å­˜çš„è·¯å¾„åŠ è½½æ¨¡å‹
model = MiniMindForCausalLM.from_pretrained("./minimind_model")

# æˆ–è€…ä»Hugging Face HubåŠ è½½
# model = MiniMindForCausalLM.from_pretrained("username/minimind")
```

3. æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ï¼š
```python
import torch
from transformers import AutoTokenizer

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained("your_tokenizer_path")

# å‡†å¤‡è¾“å…¥
input_text = "ä½ å¥½ï¼Œæˆ‘æ˜¯MiniMind"
inputs = tokenizer(input_text, return_tensors="pt")

# ç”Ÿæˆæ–‡æœ¬ï¼ˆç»§æ‰¿è‡ªGenerationMixinï¼‰
with torch.no_grad():
    outputs = model.generate(
        input_ids=inputs.input_ids,
        max_length=100,
        num_beams=4,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

# è§£ç è¾“å‡º
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

4. è®­ç»ƒæ¨¡å¼ç¤ºä¾‹ï¼š
```python
from transformers import Trainer, TrainingArguments

# è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=1000,
    save_total_limit=2,
    logging_steps=100,
)

# åˆ›å»ºTrainerï¼ˆè‡ªåŠ¨å…¼å®¹PreTrainedModelï¼‰
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

5. æ¨ç†æ¨¡å¼ç¤ºä¾‹ï¼š
```python
# è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
model.eval()

# å‡†å¤‡è¾“å…¥
input_ids = torch.tensor([[1, 2, 3, 4, 5]])  # token IDs

# å‰å‘ä¼ æ’­
with torch.no_grad():
    outputs = model(input_ids=input_ids, use_cache=True)
    
# è®¿é—®è¾“å‡ºï¼ˆCausalLMOutputWithPastæ ¼å¼ï¼‰
logits = outputs.logits              # é¢„æµ‹æ¦‚ç‡
past_key_values = outputs.past_key_values  # KVç¼“å­˜
hidden_states = outputs.last_hidden_state  # éšè—çŠ¶æ€

# é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
next_token_logits = logits[:, -1, :]
next_token_id = torch.argmax(next_token_logits, dim=-1)
```

6. é…ç½®è‡ªå®šä¹‰ç¤ºä¾‹ï¼š
```python
# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
config = MiniMindConfig(
    # åŸºç¡€æ¶æ„å‚æ•°
    hidden_size=768,
    num_attention_heads=12,
    num_hidden_layers=12,
    vocab_size=32000,
    
    # é«˜çº§ç‰¹æ€§
    use_moe=True,                    # å¯ç”¨ä¸“å®¶æ··åˆ
    n_routed_experts=8,              # ä¸“å®¶æ•°é‡
    num_experts_per_tok=2,           # æ¯tokenæ¿€æ´»ä¸“å®¶æ•°
    
    # ä¼˜åŒ–è®¾ç½®
    flash_attn=True,                 # å¯ç”¨Flash Attention
    num_key_value_heads=4,           # GQAé…ç½®
    
    # transformersæ ‡å‡†å‚æ•°
    torch_dtype="float16",           # æ¨¡å‹ç²¾åº¦
    use_cache=True,                  # å¯ç”¨KVç¼“å­˜
)

# ä¿å­˜é…ç½®
config.save_pretrained("./custom_config")
```

è¿™äº›ç¤ºä¾‹å±•ç¤ºäº†MiniMindæ¨¡å‹å¦‚ä½•å®Œå…¨å…¼å®¹transformersåº“çš„ç”Ÿæ€ç³»ç»Ÿï¼Œ
æä¾›äº†ä»æ¨¡å‹åˆ›å»ºã€è®­ç»ƒã€ä¿å­˜ã€åŠ è½½åˆ°æ¨ç†çš„å®Œæ•´å·¥ä½œæµç¨‹ã€‚
"""
